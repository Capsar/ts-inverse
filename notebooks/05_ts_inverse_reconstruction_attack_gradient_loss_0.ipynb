{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_london_smartmeter = pd.read_csv(\"./data/LondonSmartMeter/london_smart_meters_dataset_without_missing_values_first_30_consumers.csv\", index_col='Time', parse_dates=['Time'])\n",
    "df_kddcup = pd.read_csv(\"./data/KDDCup_2018/kdd_cup_2018_dataset_without_missing_values.csv\", index_col='Time', parse_dates=['Time'])\n",
    "df_electricity_321_hourly = pd.read_csv(\"./data/Electricity321Hourly/electricity_hourly_dataset.csv\", index_col='Time', parse_dates=['Time'])\n",
    "df_electricity_370 = pd.read_csv(\"./data/Electricity370/LD2011_2014_first_40_consumers.csv\", index_col='Time', parse_dates=['Time'])\n",
    "# tno_df.head()\n",
    "# print(df.columns.tolist()[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n",
      "Physical cores: 64\n",
      "Total cores: 128\n",
      "Current Frequency: 2.10Mhz\n",
      "Total: 1007.71 GB\n",
      "Available: 393.66 GB\n",
      "Used: 605.23 GB\n",
      "Percentage: 60.9%\n",
      "Distributed PyTorch available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "print(\"Total cores:\", psutil.cpu_count(logical=True))\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"Current Frequency: {cpu_freq.current:.2f}Mhz\")\n",
    "\n",
    "# RAM Information\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"Total: {svmem.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available: {svmem.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used: {svmem.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Percentage: {svmem.percent}%\")\n",
    "\n",
    "print(\"Distributed PyTorch available:\", torch.distributed.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 54 processes\n",
      "Starting attack 0 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 0, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 1 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 1, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 2 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 2, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 3 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 3, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 4 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 4, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 5 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 5, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 3, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'receptive_field': 71, 'model': 'TCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 6 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 6, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 7 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 7, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 8 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 8, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 9 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 9, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 10 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 10, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 11 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 11, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 3, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'receptive_field': 71, 'model': 'TCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 12 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 12, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n",
      "Starting attack 13 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 13, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 316}\n",
      "Starting attack 14 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 14, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n",
      "Starting attack 15 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 15, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 316}\n",
      "Starting attack 16 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 16, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n",
      "Starting attack 17 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 10, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 17, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 3, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'receptive_field': 71, 'model': 'TCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 316}\n",
      "Starting attack 18 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 18, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 19 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 19, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 20 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 20, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 21 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 21, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 22 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 22, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 569}\n",
      "torch.Size([1, 96, 1]) torch.Size([1, 96])\n",
      "Starting attack 23 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 1, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 23, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 3, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'receptive_field': 71, 'model': 'TCN_Predictor', 'num_attack_steps': 5000, 'train_dataset_size': 316}\n",
      "torch.Size([1, 48, 1]) torch.Size([1, 48])\n",
      "Starting attack 24 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 24, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 25 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 25, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 26 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 26, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 27 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 27, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 28 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 28, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 569}\n",
      "Starting attack 29 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 2, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 29, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 3, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'receptive_field': 71, 'model': 'TCN_Predictor', 'num_attack_steps': 10000, 'train_dataset_size': 316}\n",
      "Starting attack 30 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 30, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'FCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n",
      "Starting attack 31 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 31, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'FCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 316}\n",
      "Starting attack 32 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 32, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'model': 'CNN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n",
      "Starting attack 33 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 33, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'london_smartmeter', 'columns': ['T1'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 48, 'output_size': 48, 'model': 'CNN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 316}\n",
      "Starting attack 34 with config: {'logger_service': 'wandb', 'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024', 'seed': 28, 'batch_size': 4, 'device': 0, 'verbose': False, 'pool_size': 1, 'run_number': 34, 'total_variation_alpha_inputs': 0, 'total_variation_beta_targets': 0, 'after_effect': 'clamp_2', 'warmup_number_of_batches': 0, 'number_of_batches': 1, 'update_model': False, 'model_evaluation_during_attack': False, 'load_lti_model': False, 'dropout': 0.1, 'optimize_dropout': True, 'dropout_probability_regularizer': 1e-05, 'dummy_init_method': 'rand', 'attack_method': 'TS-Inverse', 'num_learn_epochs': 0, 'learn_learning_rate': 0.001, 'attack_batch_size': 32, 'inversion_batch_size': 1, 'attack_hidden_size': [768, 512], 'quantiles': [0.1, 0.3, 0.7, 0.9], 'attack_loss': 'quantile', 'inversion_model': 'ImprovedGradToInputNN_Quantile', 'attack_targets': True, 'learn_optimizer': 'adamW', 'learn_lr_decay': 'on_plateau', 'aux_dataset': None, 'one_shot_targets': True, 'inversion_regularization_term_inputs': 0, 'inversion_regularization_term_targets': 0, 'inversion_regularization_loss': 'quantile', 'inversion_dilation_loss_alpha_gamma': [0.1, 0.9], 'lower_res_term': 0, 'trend_term': 0, 'trend_loss': 'l1_mean', 'trend_reduce_lr': False, 'periodicity_term': 0, 'periodicity_loss': 'l1_mean', 'periodicity_reduce_lr': False, 'gradient_loss': 'log_cosh', 'base_num_attack_steps': 500, 'optimization_learning_rate': 0.01, 'attack_opti_optimizer': 'adam', 'attack_opti_lr_decay': 'on_plateau_10', 'clamp_dropout': 1, 'clamp_dropout_min_max': [0.0, 1.0], 'dropout_mask_init_type': 'halves', 'grad_signs_for_inputs': True, 'grad_signs_for_targets': False, 'grad_signs_for_dropouts': True, 'dataset': 'electricity_370', 'columns': ['MT_005'], 'train_stride': 24, 'validation_stride': 1, 'observation_days': 1, 'future_days': 1, 'normalize': 'minmax', 'hidden_size': 64, 'num_levels': 4, 'kernel_size': 6, 'dilation_factor': 2, 'activation': 'relu', 'use_weight_norm': True, 'init_weights': True, '_attack_step_multiplier': 10, 'features': [0], 'input_size': 96, 'output_size': 96, 'receptive_field': 151, 'model': 'TCN_Predictor', 'num_attack_steps': 20000, 'train_dataset_size': 569}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m dataset_config \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m     {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39melectricity_370\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39m# },    \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m ]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m model_config \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m     {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_model\u001b[39m\u001b[39m'\u001b[39m: FCN_Predictor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=181'>182</a>\u001b[0m     \u001b[39m# }\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m ]\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=184'>185</a>\u001b[0m start_multi_process(global_config, attack_config, dataset_config, model_config, global_config[\u001b[39m'\u001b[39m\u001b[39mpool_size\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m pool_size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m search_args:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhinton.tnods.nl/home/caspar.meijer/federatedlearning/05_ts_inverse_reconstruction_attack_gradient_loss_0.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         AttackTSInverseWorker(args[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrun_number\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mworker_process(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/federatedlearning/src/workers/attack_ts_inverse_worker.py:43\u001b[0m, in \u001b[0;36mAttackTSInverseWorker.worker_process\u001b[0;34m(self, c, d_c, m_c, fam_c)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_logger_object(final_config)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_attack(model, train_dataloader, final_config)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_attack(model, config\u001b[39m=\u001b[39mfinal_config)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_end_logger_object()\n",
      "File \u001b[0;32m~/federatedlearning/src/workers/attack_dlg_invg_dia_worker.py:75\u001b[0m, in \u001b[0;36mAttackBaselineWorker.start_attack\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m     72\u001b[0m     batch_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_batch_inputs[batch_number]\n\u001b[1;32m     73\u001b[0m     batch_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_batch_targets[batch_number]\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattack_batch(model, config, batch_number, original_dy_dx, dummy_inputs, dummy_targets, batch_inputs, batch_targets)\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/federatedlearning/src/workers/attack_ts_inverse_worker.py:178\u001b[0m, in \u001b[0;36mAttackTSInverseWorker.attack_batch\u001b[0;34m(self, model, config, batch_number, original_dy_dx, dummy_inputs, dummy_targets, batch_inputs, batch_targets)\u001b[0m\n\u001b[1;32m    174\u001b[0m             dropout_layer\u001b[39m.\u001b[39mdo_mask\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39msign_()\n\u001b[1;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m dy_dx_loss\n\u001b[0;32m--> 178\u001b[0m dy_dx_loss \u001b[39m=\u001b[39m dummy_optimizer\u001b[39m.\u001b[39mstep(closure)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_effect(config, model, dummy_inputs, dummy_targets, attack_step)\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschedular_step(config[\u001b[39m'\u001b[39m\u001b[39mattack_opti_lr_decay\u001b[39m\u001b[39m'\u001b[39m], dummy_schedular, attack_metrics, dy_dx_loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/tno_fl_cuda/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tno_fl_cuda/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/tno_fl_cuda/lib/python3.11/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/federatedlearning/src/workers/attack_ts_inverse_worker.py:135\u001b[0m, in \u001b[0;36mAttackTSInverseWorker.attack_batch.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mTCN\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mname \u001b[39mand\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39moptimize_dropout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mdropout_probability_regularizer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m dropout_layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mget_dropout_layers():\n\u001b[0;32m--> 135\u001b[0m         dy_dx_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mdropout_probability_regularizer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dropout_layer\u001b[39m.\u001b[39mdo_mask\u001b[39m.\u001b[39mmean()) \u001b[39m-\u001b[39m dropout_layer\u001b[39m.\u001b[39mp)\u001b[39m.\u001b[39mabs()\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m inversion_model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (config[\u001b[39m'\u001b[39m\u001b[39minversion_regularization_term_inputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39minversion_regularization_term_targets\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    138\u001b[0m     dy_dx_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearned_prior_regularization(dummy_inputs, dummy_targets, regularization_inputs, regularization_targets, config)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from ts_inverse.models import FCN_Predictor, CNN_Predictor, GRU_Predictor, JitGRU_Predictor, CNNJitGRU_Predictor, TCN_Predictor, JitSeq2Seq_Predictor\n",
    "from ts_inverse.utils import grid_search_params\n",
    "from ts_inverse.workers import AttackTSInverseWorker\n",
    "\n",
    "\n",
    "def start_multi_process(g_config, a_config, d_config, m_config, pool_size):\n",
    "    search_args = []\n",
    "    search_configs = list(grid_search_params(g_config))\n",
    "    search_attack_configs = list(grid_search_params(a_config))\n",
    "    search_dataset_settings = list(grid_search_params(d_config))\n",
    "    search_model_settings = list(grid_search_params(m_config))\n",
    "    for original_g_config in search_configs:\n",
    "        for a_config in search_attack_configs:\n",
    "            g_config = deepcopy(original_g_config)\n",
    "            g_config.update(a_config)\n",
    "            for m_config in search_model_settings:\n",
    "                for d_config in search_dataset_settings:\n",
    "                    fa_models_config = {\n",
    "                        'features': [[0]],\n",
    "                        'input_size': d_config['observation_days'],\n",
    "                        'output_size': d_config['future_days'],\n",
    "                    }\n",
    "                    search_for_all_models_settings = list(grid_search_params(fa_models_config))\n",
    "                    for fa_models_config in search_for_all_models_settings:\n",
    "                        g_config['run_number'] = len(search_args)\n",
    "                        args = (g_config, d_config, m_config, fa_models_config)\n",
    "                        search_args.append(deepcopy(args))\n",
    "\n",
    "    print(f\"Starting {len(search_args)} processes\")\n",
    "    if pool_size == 1:\n",
    "        for args in search_args:\n",
    "            AttackTSInverseWorker(args[0]['run_number']).worker_process(*args)\n",
    "\n",
    "\n",
    "global_config = {\n",
    "    'logger_service': 'wandb',\n",
    "    'experiment_name': 'ts-inverse_fixed_gradient_loss_12-6-2024',\n",
    "    'seed': [10, 28, 43], # 28, 80, 71],\n",
    "    'batch_size': [1, 2, 4],\n",
    "    'device': 0,\n",
    "    'verbose': False,\n",
    "    'pool_size': 1,\n",
    "    'run_number': -1,\n",
    "    'total_variation_alpha_inputs': 0, \n",
    "    'total_variation_beta_targets': 0,\n",
    "    'after_effect': 'none',\n",
    "    'warmup_number_of_batches': 0,\n",
    "    'number_of_batches': 1,\n",
    "    'update_model': False, # Update the model in generating gradients from training data\n",
    "    'model_evaluation_during_attack': False, # Baselines do not consider this\n",
    "    'load_lti_model': False,\n",
    "\n",
    "    'dropout': 0,\n",
    "    'optimize_dropout': False,\n",
    "    'dropout_probability_regularizer': 0,\n",
    "    'dummy_init_method': 'rand',\n",
    "}\n",
    "\n",
    "attack_config = [\n",
    "    {\n",
    "        'attack_method': 'TS-Inverse',\n",
    "        # invert attack\n",
    "        'num_learn_epochs': 0,\n",
    "        'learn_learning_rate': 1e-3, \n",
    "        'attack_batch_size': 32,\n",
    "        'inversion_batch_size': 1, # global_config['batch_size'],\n",
    "        'attack_hidden_size': [[768, 512]],\n",
    "        'quantiles': [[0.1, 0.3, 0.7, 0.9]],\n",
    "        'attack_loss': ['quantile'],\n",
    "        'inversion_model': 'ImprovedGradToInputNN_Quantile', \n",
    "        'attack_targets': True,\n",
    "        'learn_optimizer': 'adamW',\n",
    "        'learn_lr_decay': ['on_plateau'],\n",
    "        'aux_dataset': None,\n",
    "        'one_shot_targets': True,\n",
    "\n",
    "        ## Inversion regularization in optimization attack\n",
    "        'inversion_regularization_term_inputs': [0], \n",
    "        'inversion_regularization_term_targets': [0],\n",
    "        'inversion_regularization_loss': ['quantile'],\n",
    "\n",
    "        'lower_res_term': [0],\n",
    "        'trend_term': [0],\n",
    "        'trend_loss': ['l1_mean'],\n",
    "        'trend_reduce_lr': [False],\n",
    "        'periodicity_term': [0],\n",
    "        'periodicity_loss': ['l1_mean'],\n",
    "        'periodicity_reduce_lr': [False],\n",
    "\n",
    "\n",
    "        ## Optimization attack\n",
    "        'gradient_loss': ['log_cosh'], #['l1', 'cosine_dia', 'euclidean', '1_l1norm_1_cosine', '1_l2norm_1_cosine'], \n",
    "        'base_num_attack_steps': 500,\n",
    "        'after_effect': 'clamp_2',\n",
    "        'optimization_learning_rate': 0.01, \n",
    "        'attack_opti_optimizer': ['adam'],\n",
    "        'attack_opti_lr_decay': ['on_plateau_10'],\n",
    "        'optimize_dropout': True,\n",
    "        'clamp_dropout': 1,\n",
    "        'clamp_dropout_min_max': [[0.0, 1.0]],\n",
    "        'dropout_probability_regularizer': [1e-5], #1e-6, 1e-7, 1e-8],\n",
    "        'dropout_mask_init_type': 'halves', #['bernoulli', 'halves', 'uniform', 'p', '1-p'], #['halves', 'bernoulli'],\n",
    "        'grad_signs_for_inputs': True,\n",
    "        'grad_signs_for_targets': False,\n",
    "        'grad_signs_for_dropouts': True, #[False, True],\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_config = [\n",
    "    {\n",
    "        'dataset': 'electricity_370',\n",
    "        'columns': [df_electricity_370.columns.tolist()[4:5]],\n",
    "        'train_stride': 24,\n",
    "        'validation_stride': 1,\n",
    "        'observation_days': 1,\n",
    "        'future_days': 1,\n",
    "        'normalize': 'minmax',\n",
    "    },\n",
    "    {\n",
    "        'dataset': 'london_smartmeter',\n",
    "        'columns': [df_london_smartmeter.columns.tolist()[:1]],\n",
    "        'train_stride': 24, # Is the strides that are attacked\n",
    "        'validation_stride': 1, # Is the stride that is used for training the invertion model\n",
    "        'observation_days': 1,\n",
    "        'future_days': 1,\n",
    "        'normalize': 'minmax',\n",
    "    },\n",
    "    # {\n",
    "    #     'dataset': 'kddcup',\n",
    "    #     'columns': [df_kddcup.columns.tolist()[:1]],\n",
    "    #     'train_stride': 24,\n",
    "    #     'validation_stride': 1,\n",
    "    #     'observation_days': 5,\n",
    "    #     'future_days': 2,\n",
    "    #     'normalize': 'minmax',\n",
    "    # },    \n",
    "]\n",
    "\n",
    "model_config = [\n",
    "    {\n",
    "        '_model': FCN_Predictor,\n",
    "        'hidden_size': 64,\n",
    "        '_attack_step_multiplier': 10,\n",
    "    },\n",
    "    {\n",
    "        '_model': CNN_Predictor,\n",
    "        'hidden_size': 64,\n",
    "        '_attack_step_multiplier': 10,\n",
    "    },\n",
    "    {\n",
    "        '_model': TCN_Predictor,\n",
    "        'hidden_size': 64,\n",
    "        'num_levels': 0,\n",
    "        'kernel_size': 6,\n",
    "        'dilation_factor': 2,\n",
    "        'activation': 'relu',\n",
    "        'use_weight_norm': True,\n",
    "        'init_weights': True,\n",
    "        'dropout': 0.1,\n",
    "        '_attack_step_multiplier': 10,\n",
    "    },\n",
    "    # {\n",
    "    #     '_model': JitGRU_Predictor,\n",
    "    #     'hidden_size': 64,\n",
    "    #     '_attack_step_multiplier': 10,\n",
    "    # },\n",
    "    # {\n",
    "    #     '_model': JitSeq2Seq_Predictor,\n",
    "    #     'hidden_size': 64,\n",
    "    #     '_attack_step_multiplier': 10,\n",
    "    # }\n",
    "]\n",
    "\n",
    "start_multi_process(global_config, attack_config, dataset_config, model_config, global_config['pool_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
