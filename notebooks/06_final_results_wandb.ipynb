{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from ts_inverse.attack_time_series_utils import SMAPELoss\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.transforms import Bbox\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "def gather_run_histories(columns, rows, lines, filters, runs, y_axis):\n",
    "    unique_columns, unique_rows, unique_lines = set(), set(), set()\n",
    "    for run in runs:\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) for k, v in filter_dict.items() if k in run.config]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            continue\n",
    "\n",
    "        unique_columns.add(run.config[columns])\n",
    "        unique_rows.add(run.config[rows])\n",
    "        unique_lines.add(run.config[lines])\n",
    "\n",
    "    unique_columns = sorted(list(unique_columns))\n",
    "    unique_rows = sorted(list(unique_rows))\n",
    "    unique_lines = sorted(list(unique_lines))\n",
    "    print(unique_columns, unique_rows, unique_lines)\n",
    "\n",
    "    series_dict = {}\n",
    "\n",
    "    for run in runs:\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) if k in run.config else False for k, v in filter_dict.items()]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            continue\n",
    "\n",
    "        row_label = run.config[rows]\n",
    "        column_label = run.config[columns]\n",
    "\n",
    "        i = unique_rows.index(row_label)\n",
    "        j = unique_columns.index(column_label)\n",
    "\n",
    "        historys = run.history(keys=y_axis).set_index(\"_step\")\n",
    "        mean_series = historys.mean(axis=1)\n",
    "        mean_series.name = run.config[\"seed\"]\n",
    "        if (i, j) not in series_dict:\n",
    "            series_dict[(i, j)] = {}\n",
    "\n",
    "        if run.config[lines] not in series_dict[(i, j)]:\n",
    "            series_dict[(i, j)][run.config[lines]] = mean_series.to_frame()\n",
    "        else:\n",
    "            series_dict[(i, j)][run.config[lines]][mean_series.name] = mean_series\n",
    "\n",
    "    y_lines = [f\"{s.split('/')[0]} {s.split('/')[1].upper()}\" for s in y_axis]\n",
    "    title = f'Comparing {str.join(\", \", y_lines)} over {columns} and {rows} for different {lines}'\n",
    "\n",
    "    return unique_columns, unique_rows, series_dict, title\n",
    "\n",
    "\n",
    "def plot_metrics_in_grid(unique_columns, unique_rows, series_dict, title, label_prefix, x_limits):\n",
    "    plot_size = 7\n",
    "    fig, axes = plt.subplots(\n",
    "        len(unique_rows), len(unique_columns), figsize=(plot_size * len(unique_columns), plot_size * len(unique_rows))\n",
    "    )\n",
    "    for (i, j), graph_data in series_dict.items():\n",
    "        row_label, column_label = unique_rows[i], unique_columns[j]\n",
    "        for line_label, df_series in graph_data.items():\n",
    "            df_series.ffill(inplace=True)\n",
    "            mean_series = df_series.mean(axis=1)\n",
    "            std_series = df_series.std(axis=1)\n",
    "            axes[i, j].fill_between(mean_series.index, mean_series - std_series, mean_series + std_series, alpha=0.2)\n",
    "            axes[i, j].plot(mean_series, label=f\"{label_prefix}{line_label}\")\n",
    "\n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel(row_label)\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(column_label)\n",
    "        axes[i, j].set_ylim(0)\n",
    "        axes[i, j].set_xlim(*x_limits)\n",
    "        axes[i, j].legend()\n",
    "        # axes[i, j].set_yscale('log')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"./out/plots/baselines/grid_{title}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics(unique_columns, unique_rows, series_dict, title, label_prefix, x_limits):\n",
    "    # Plot the metrics in a single plot and average over seeds, columns and rows, just like the grid plot\n",
    "    # Average the different seeds, columns and rows such that a pingle plot with multiple lines of just line_label is created\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    line_dict = {}\n",
    "    for (i, j), graph_data in series_dict.items():\n",
    "        row_label, column_label = unique_rows[i], unique_columns[j]\n",
    "        for line_label, df_series in graph_data.items():\n",
    "            if line_label not in line_dict:\n",
    "                line_dict[line_label] = [df_series]\n",
    "            else:\n",
    "                line_dict[line_label].append(df_series)\n",
    "\n",
    "    for line_label, df_series_list in line_dict.items():\n",
    "        df_series = pd.concat(df_series_list, axis=1, join=\"inner\")\n",
    "        # df_series.ffill(inplace=True)\n",
    "        mean_series = df_series.mean(axis=1)\n",
    "        std_series = df_series.std(axis=1)\n",
    "        ax.fill_between(mean_series.index, mean_series - std_series, mean_series + std_series, alpha=0.2)\n",
    "        ax.plot(mean_series, label=f\"{label_prefix}{line_label}\")\n",
    "\n",
    "    ax.set_xlim(*x_limits)\n",
    "    ax.legend()\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"./out/plots/baselines/all_{title}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# CREATE TABLES\n",
    "def gather_final_metrics_by_parameters(\n",
    "    columns, rows, metrics, variable, filters, runs, variables_other_sorted=None, specific_seed=None\n",
    "):\n",
    "    history_dict = {}\n",
    "    if isinstance(runs, list):\n",
    "        runs = [run for run_list in runs for run in run_list]\n",
    "\n",
    "    runs_found = 0\n",
    "    for run in runs:\n",
    "        if specific_seed is not None and run.config[\"seed\"] != specific_seed:\n",
    "            continue\n",
    "\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) if k in run.config else False for k, v in filter_dict.items()]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            continue\n",
    "\n",
    "        run_column = str(run.config[columns]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        run_row = str(run.config[rows]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        run_variable = str(run.config[variable]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        run_metric = [run.summary[metric] for metric in metrics]\n",
    "        # run_metric = { metric: run.summary[metric] for metric in metrics}\n",
    "        run_seed = run.config[\"seed\"]\n",
    "\n",
    "        if (run_column, run_row, run_variable) not in history_dict:\n",
    "            history_dict[(run_column, run_row, run_variable)] = [run_metric]\n",
    "        else:\n",
    "            history_dict[(run_column, run_row, run_variable)].append(run_metric)\n",
    "        runs_found += 1\n",
    "\n",
    "    print(f\"Found {runs_found} runs with metrics: {history_dict}\")\n",
    "\n",
    "    unique_columns = sorted(list(set([column for column, _, _ in history_dict.keys()])))\n",
    "    unique_rows = sorted(list(set([row for _, row, _ in history_dict.keys()])))\n",
    "    unique_variables = sorted(list(set([variable for _, _, variable in history_dict.keys()])))\n",
    "    if variables_other_sorted is not None:\n",
    "        unique_variables = variables_other_sorted\n",
    "\n",
    "    print(unique_columns, unique_rows, unique_variables)\n",
    "\n",
    "    # Create a table with the average metrics for each variable\n",
    "    table = np.zeros((len(unique_columns), len(unique_rows), len(unique_variables), len(metrics) * 2))\n",
    "    for j, row in enumerate(unique_rows):\n",
    "        for i, column in enumerate(unique_columns):\n",
    "            for k, variable in enumerate(unique_variables):\n",
    "                if (column, row, variable) in history_dict:\n",
    "                    table[i, j, k, 0:2] = np.mean(history_dict[(column, row, variable)], axis=0)\n",
    "                    if len(history_dict[(column, row, variable)]) > 1:\n",
    "                        table[i, j, k, 2:4] = np.std(history_dict[(column, row, variable)], axis=0)\n",
    "                    else:\n",
    "                        table[i, j, k, 2:4] = (\n",
    "                            np.ones(2) * -1\n",
    "                        )  # If there is only one seed, the std is -1 to indicate that there is no std\n",
    "                else:\n",
    "                    table[i, j, k, :] = np.nan\n",
    "    return table, unique_columns, unique_rows, unique_variables\n",
    "\n",
    "\n",
    "def print_latex_table_input_target(table, unique_columns, unique_rows, unique_variables, variable_name=\"\"):\n",
    "    print(\"\\\\begin{table*}[]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\resizebox{\\\\linewidth}{!}{\")\n",
    "    print(\"\\\\begin{tabular}{l|l\" + \"|cc\" * len(unique_columns) + \"}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\n",
    "        \"      & Dataset                 & \\\\multicolumn{2}{c}{Electricity 370} & \\\\multicolumn{2}{c}{KDDCup} & \\\\multicolumn{2}{c}{London Smartmeter} & \\\\multicolumn{2}{c}{Proprietary} \\\\\\\\\"\n",
    "    )\n",
    "    dataset_headers = \" & \".join([f\"\\\\multicolumn{{2}}{{c}}{{{col}}}\" for col in unique_columns])\n",
    "    print(f\"      & REPLACE                 & {dataset_headers} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "\n",
    "    # print(f\"Model & {variable_name} & Input            & Target           & Input       & Target       & Input             & Target            & Input            & Target           \\\\\\\\\")\n",
    "    print(f\"Model & {variable_name} & {'           & '.join(['Input & Target'] * len(unique_columns))} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "\n",
    "    def add_bold_if_min(number_value, string, minimum):\n",
    "        if number_value == minimum:\n",
    "            return \"\\\\textbf{\" + str(string) + \"}\"\n",
    "        return str(string)\n",
    "\n",
    "    def format_mean_std_value(value, std):\n",
    "        # \\textbf{0.573$\\pm$0.160}\n",
    "        # $\\mathbf{0.573}_{\\mathbf{0.160}}$\n",
    "        if value < 1e-3:\n",
    "            if std > 0:\n",
    "                std_str = f\"{std:.2f}\"  # .lstrip('0')  # Remove leading zero\n",
    "                return (f\"{value:.1E}$_\" + \"{\" + std_str + \"}$\").replace(\"E\", \"e\")\n",
    "            return f\"{value:.1E}\".replace(\"E\", \"e\")\n",
    "        if std > 0:\n",
    "            std_str = f\"{std:.2f}\"  # .lstrip('0')  # Remove leading zero\n",
    "            return f\"{value:.3f}$_\" + \"{\" + std_str + \"}$\"\n",
    "        return f\"{value:.3f}\"\n",
    "\n",
    "    # def format_mean_std_value(value, std):\n",
    "    #     # \\textbf{0.573$\\pm$0.160}\n",
    "    #     # $\\mathbf{0.573}_{\\mathbf{0.160}}$\n",
    "    #     if value < 1e-3:\n",
    "    #         if std > 0:\n",
    "    #             return f\"{value:.1E}$\\pm${std:.1E}\".replace('E', 'e')\n",
    "    #         return f\"{value:.1E}\".replace('E', 'e')\n",
    "    #     if std > 0:\n",
    "    #         return f\"{value:.3f}$\\pm${std:.3f}\"\n",
    "    #     return f\"{value:.3f}\"\n",
    "\n",
    "    for j, row in enumerate(unique_rows):\n",
    "        for k, variable in enumerate(unique_variables):\n",
    "            if k == 0:\n",
    "                row_values = [f\"{row.replace('_Predictor', '')}\\t& {variable}\\t\"]\n",
    "            else:\n",
    "                row_values = [f\"\\t& {variable}\\t\"]\n",
    "            for i, column in enumerate(unique_columns):\n",
    "                # Assuming last dimension is for mean values and we want the first element\n",
    "                input_mean_value, target_mean_value = table[i, j, k, 0], table[i, j, k, 1]\n",
    "                input_std_value, target_std_value = table[i, j, k, 2], table[i, j, k, 3]\n",
    "                # If the input mean or target mean are the minimum value then it should be printed in bold with \\textbf{}\n",
    "                if not np.isnan(input_mean_value):\n",
    "                    row_string_value = f\"\"\n",
    "                    row_string_value += add_bold_if_min(\n",
    "                        input_mean_value, format_mean_std_value(input_mean_value, input_std_value), np.nanmin(table[i, j, :, 0])\n",
    "                    )\n",
    "                    row_string_value += \" & \"\n",
    "                    row_string_value += add_bold_if_min(\n",
    "                        target_mean_value,\n",
    "                        format_mean_std_value(target_mean_value, target_std_value),\n",
    "                        np.nanmin(table[i, j, :, 1]),\n",
    "                    )\n",
    "                    row_values.append(row_string_value)\n",
    "                else:\n",
    "                    row_values.append(\"N/A & N/A\")\n",
    "            print(\" & \".join(row_values) + \" \\\\\\\\\")\n",
    "        if j != len(unique_rows) - 1:\n",
    "            print(\"\\\\midrule\")\n",
    "\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}}\")\n",
    "    print(\"\\\\caption{}\")\n",
    "    print(\"\\\\end{table*}\")\n",
    "\n",
    "\n",
    "# PLOT RECONSTRUCTIONS\n",
    "def gather_run_reconstructions(dataset_seed, columns, rows, variables, filters, runs, replace_dict={}):\n",
    "    def dataframe_keys_and_should_skip_run(run):\n",
    "        if run.config[\"dataset\"] not in dataset_seed or run.config[\"seed\"] != dataset_seed[run.config[\"dataset\"]]:\n",
    "            return [], True\n",
    "\n",
    "        dataframe_keys = []\n",
    "        for key in run.summary.keys():\n",
    "            if \"dataframe\" in key:\n",
    "                dataframe_keys.append(key)\n",
    "        if len(dataframe_keys) == 0:\n",
    "            return [], True\n",
    "\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) if k in run.config else False for k, v in filter_dict.items()]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            return [], True\n",
    "        return dataframe_keys, False\n",
    "\n",
    "    def replace_predictor_name(runs_config, name):\n",
    "        run_config_value = runs_config[name]\n",
    "        for key, value in replace_dict.items():\n",
    "            if isinstance(run_config_value, str):\n",
    "                run_config_value = run_config_value.replace(key, value)\n",
    "        if name == \"dataset\":\n",
    "            return f\"{run_config_value} ({dataset_seed[runs_config[name]]})\"\n",
    "        if name == \"model\":\n",
    "            return run_config_value.replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        return run_config_value\n",
    "\n",
    "    reconstructed_data_dict = {}\n",
    "    if isinstance(runs, list):\n",
    "        runs = [run for run_list in runs for run in run_list]\n",
    "\n",
    "    for run in runs:\n",
    "        dataframe_keys, should_skip_run = dataframe_keys_and_should_skip_run(run)\n",
    "        if should_skip_run:\n",
    "            continue\n",
    "\n",
    "        run_column = replace_predictor_name(run.config, columns)\n",
    "        run_row = replace_predictor_name(run.config, rows)\n",
    "        run_variable = replace_predictor_name(run.config, variables)\n",
    "\n",
    "        dataframe_info = run.summary[dataframe_keys[0]]\n",
    "        file = run.file(dataframe_info[\"path\"])\n",
    "        if not os.path.exists(file.name):\n",
    "            file.download(replace=True)\n",
    "        with open(file.name, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        reconstructed_data = pd.DataFrame(data=data[\"data\"], columns=data[\"columns\"])\n",
    "        if (run_column, run_row, run_variable) not in reconstructed_data_dict:\n",
    "            reconstructed_data_dict[(run_column, run_row, run_variable)] = reconstructed_data\n",
    "\n",
    "    print(reconstructed_data_dict.keys())\n",
    "\n",
    "    unique_columns = sorted(list(set([column for column, _, _ in reconstructed_data_dict.keys()])))\n",
    "    unique_rows = sorted(list(set([row for _, row, _ in reconstructed_data_dict.keys()])))\n",
    "    unique_variables = sorted(list(set([variable for _, _, variable in reconstructed_data_dict.keys()])))\n",
    "\n",
    "    return reconstructed_data_dict, unique_columns, unique_rows, unique_variables\n",
    "\n",
    "\n",
    "def plot_single_batch_reconstructions_in_grid(\n",
    "    series_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    unique_variables,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "):\n",
    "    if not series_dict:\n",
    "        print(\"No series given, skipping plotting\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows) * len(unique_variables)))\n",
    "    subfigs = fig.subfigures(len(unique_rows), len(unique_columns))\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        subfigs = np.array([[subfigs]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        subfigs = np.array([subfigs])\n",
    "    elif len(unique_columns) == 1:\n",
    "        subfigs = np.array([subfigs]).T\n",
    "\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            subfig = subfigs[i, j]\n",
    "            subfig.subplots_adjust(wspace=0, hspace=0, left=0.05, right=0.99, top=0.9, bottom=0.05)\n",
    "            axes_1 = subfig.subplots(len(unique_variables), 1, sharex=True, sharey=True)\n",
    "            if len(unique_variables) == 1:\n",
    "                axes_1 = [axes_1]\n",
    "            for k, variable in enumerate(unique_variables):\n",
    "                if (column_label, row_label, variable) in series_dict:\n",
    "                    reconstructed_data = series_dict[(column_label, row_label, variable)]\n",
    "                    n_batches = len(reconstructed_data.columns) // 4\n",
    "                    for b in range(n_batches):\n",
    "                        df_plot = reconstructed_data[[f\"batch_inputs_{b}_0\", f\"batch_targets_{b}\"]]\n",
    "                        batch_colors = [colors[(2 * b) % 10], colors[(2 * b + 2) % 10]]\n",
    "                        df_plot.plot(ax=axes_1[k], legend=False, color=batch_colors)\n",
    "\n",
    "                    for b in range(n_batches):\n",
    "                        df_plot = reconstructed_data[[f\"dummy_inputs_{b}_0\", f\"dummy_targets_{b}\"]]\n",
    "                        dummy_colors = [colors[(2 * b + 1) % 10], colors[(2 * b + 3) % 10]]\n",
    "                        df_plot.plot(ax=axes_1[k], legend=False, style=[\"--\", \"--\"], color=dummy_colors)\n",
    "\n",
    "                    # reconstructed_data.plot(ax=axes_1[k], legend=False)\n",
    "                    dummy_handle = mlines.Line2D([], [], color=\"none\", label=variable)\n",
    "                    axes_1[k].legend(\n",
    "                        handles=[dummy_handle],\n",
    "                        loc=legend_loc,\n",
    "                        handlelength=0,\n",
    "                        handletextpad=0,\n",
    "                        fancybox=True,\n",
    "                        fontsize=legend_font_size,\n",
    "                    )\n",
    "\n",
    "                axes_1[k].set_ylim(-0.1, 1.1)\n",
    "                axes_1[k].set_yticks([])\n",
    "                axes_1[k].set_xticks([])\n",
    "\n",
    "            if i == 0:\n",
    "                subfig.text(0.5, 1.0 + column_offset, column_label, ha=\"center\", va=\"top\")\n",
    "            if j == 0:\n",
    "                subfig.text(0.0, 0.5, row_label, ha=\"left\", va=\"center\", fontsize=8, rotation=90)\n",
    "\n",
    "    fig.savefig(f\"./out/plots/reconstructions/grid_reconstructions_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_batch_sample_mapping(original_data, dummy_data):\n",
    "    batch_size = original_data.shape[0]\n",
    "    sample_mapping = np.arange(0, batch_size)\n",
    "    for i in range(batch_size):\n",
    "        smallest_loss = float(\"inf\")\n",
    "        for j in range(batch_size):\n",
    "            loss = (\n",
    "                F.l1_loss(original_data[i], dummy_data[j]).detach().item()\n",
    "            )  # instead of MSE because L1 is less sensitive to outliers\n",
    "            if loss < smallest_loss:\n",
    "                smallest_loss = loss\n",
    "                sample_mapping[i] = j\n",
    "    # Check if sample_mapping contains duplicates; if so, reset it\n",
    "    if len(np.unique(sample_mapping)) != batch_size:\n",
    "        sample_mapping = np.arange(0, batch_size)\n",
    "    return sample_mapping\n",
    "\n",
    "\n",
    "def get_batch_sample_mapping_from_dataframe(reconstructed_data, batch_size):\n",
    "    original_inputs = torch.tensor(\n",
    "        reconstructed_data[[f\"batch_inputs_{i}_0\" for i in range(batch_size)]].dropna().values.T\n",
    "    ).unsqueeze(-1)\n",
    "    dummy_inputs = torch.tensor(\n",
    "        reconstructed_data[[f\"dummy_inputs_{i}_0\" for i in range(batch_size)]].dropna().values.T\n",
    "    ).unsqueeze(-1)\n",
    "    original_targets = torch.tensor(\n",
    "        reconstructed_data[[f\"batch_targets_{i}\" for i in range(batch_size)]].dropna().values.T\n",
    "    ).unsqueeze(-1)\n",
    "    dummy_targets = torch.tensor(\n",
    "        reconstructed_data[[f\"dummy_targets_{i}\" for i in range(batch_size)]].dropna().values.T\n",
    "    ).unsqueeze(-1)\n",
    "    # print(original_inputs.shape, dummy_inputs.shape, original_targets.shape, dummy_targets.shape)\n",
    "\n",
    "    standard_mapping = np.arange(0, batch_size)\n",
    "    input_sample_mapping = get_batch_sample_mapping(original_inputs, dummy_inputs)\n",
    "    target_sample_mapping = get_batch_sample_mapping(original_targets, dummy_targets)\n",
    "\n",
    "    sample_mapping = np.arange(0, batch_size)\n",
    "    if not (standard_mapping == input_sample_mapping).all():\n",
    "        sample_mapping = input_sample_mapping\n",
    "    if (standard_mapping == input_sample_mapping).all() and not (standard_mapping == target_sample_mapping).all():\n",
    "        sample_mapping = target_sample_mapping\n",
    "    # if not (standard_mapping == input_sample_mapping).all() and not (standard_mapping == target_sample_mapping).all():\n",
    "    #     if not (input_sample_mapping == target_sample_mapping).all():\n",
    "    #         raise ValueError('Input and target sample mappings are not equal while being different from the standard mapping.')\n",
    "    return sample_mapping, original_inputs, dummy_inputs, original_targets, dummy_targets\n",
    "\n",
    "\n",
    "def gather_final_metrics_from_reconstructions_by_parameters(\n",
    "    columns, rows, metric, variable, filters, runs, variables_other_sorted=None\n",
    "):\n",
    "    history_dict = {}\n",
    "    if isinstance(runs, list):\n",
    "        runs = [run for run_list in runs for run in run_list]\n",
    "\n",
    "    runs_found = 0\n",
    "    for run in runs:\n",
    "        dataframe_keys = []\n",
    "        for key in run.summary.keys():\n",
    "            if \"dataframe\" in key and not \"quantile\" in key:  # Exclude quantile dataframes\n",
    "                dataframe_keys.append(key)\n",
    "        if len(dataframe_keys) == 0:\n",
    "            continue\n",
    "\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) if k in run.config else False for k, v in filter_dict.items()]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            continue\n",
    "\n",
    "        run_column = str(run.config[columns]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        run_row = str(run.config[rows]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        run_variable = str(run.config[variable]).replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "\n",
    "        dataframe_info = run.summary[dataframe_keys[0]]\n",
    "        file = run.file(dataframe_info[\"path\"])\n",
    "        if not os.path.exists(file.name):\n",
    "            file.download(replace=True)\n",
    "        with open(file.name, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        reconstructed_data = pd.DataFrame(data=data[\"data\"], columns=data[\"columns\"])\n",
    "\n",
    "        sample_mapping, original_inputs, dummy_inputs, original_targets, dummy_targets = get_batch_sample_mapping_from_dataframe(\n",
    "            reconstructed_data, run.config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        # Calculate the metrics\n",
    "        run_metric = [\n",
    "            metric(dummy_inputs[sample_mapping], original_inputs).item(),\n",
    "            metric(dummy_targets[sample_mapping], original_targets).item(),\n",
    "        ]\n",
    "\n",
    "        if (run_column, run_row, run_variable) not in history_dict:\n",
    "            history_dict[(run_column, run_row, run_variable)] = [run_metric]\n",
    "        else:\n",
    "            history_dict[(run_column, run_row, run_variable)].append(run_metric)\n",
    "        runs_found += 1\n",
    "\n",
    "    print(f\"Found {runs_found} runs with reconstructions\")\n",
    "    unique_columns = sorted(list(set([column for column, _, _ in history_dict.keys()])))\n",
    "    unique_rows = sorted(list(set([row for _, row, _ in history_dict.keys()])))\n",
    "    unique_variables = sorted(list(set([variable for _, _, variable in history_dict.keys()])))\n",
    "    if variables_other_sorted is not None:\n",
    "        unique_variables = variables_other_sorted\n",
    "\n",
    "    print(unique_columns, unique_rows, unique_variables)\n",
    "\n",
    "    # Create a table with the average metrics for each variable\n",
    "    table = np.zeros((len(unique_columns), len(unique_rows), len(unique_variables), 4))\n",
    "    for j, row in enumerate(unique_rows):\n",
    "        for i, column in enumerate(unique_columns):\n",
    "            for k, variable in enumerate(unique_variables):\n",
    "                if (column, row, variable) in history_dict:\n",
    "                    table[i, j, k, 0:2] = np.mean(history_dict[(column, row, variable)], axis=0)\n",
    "                    if len(history_dict[(column, row, variable)]) > 1:\n",
    "                        table[i, j, k, 2:4] = np.std(history_dict[(column, row, variable)], axis=0)\n",
    "                    else:\n",
    "                        table[i, j, k, 2:4] = (\n",
    "                            np.ones(2) * -1\n",
    "                        )  # If there is only one seed, the std is -1 to indicate that there is no std\n",
    "                else:\n",
    "                    table[i, j, k, :] = np.nan\n",
    "    return table, unique_columns, unique_rows, unique_variables\n",
    "\n",
    "\n",
    "def plot_multi_batch_reconstructions_in_grid(\n",
    "    series_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    unique_batch_sizes,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "    external_fig=None,\n",
    "):\n",
    "    if not series_dict:\n",
    "        print(\"No Dictionary provided, thuse not plotting multi batch reconstructions in grid!\")\n",
    "        return\n",
    "\n",
    "    if external_fig is not None:\n",
    "        fig = external_fig\n",
    "    else:\n",
    "        fig = plt.figure(\n",
    "            figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows) * sum(unique_batch_sizes))\n",
    "        )\n",
    "    subfigs = fig.subfigures(len(unique_rows), len(unique_columns))\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        subfigs = np.array([[subfigs]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        subfigs = np.array([subfigs])\n",
    "    elif len(unique_columns) == 1:\n",
    "        subfigs = np.array([subfigs]).T\n",
    "\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            subfig = subfigs[i, j]\n",
    "\n",
    "            batch_size = [bs for bs in unique_batch_sizes if (column_label, row_label, bs) in series_dict][0]\n",
    "\n",
    "            subfig.subplots_adjust(wspace=0, hspace=0, left=0.05, right=0.99, top=0.9, bottom=0.05)\n",
    "            axes = subfig.subplots(max(unique_batch_sizes), 1, sharex=True, sharey=True)\n",
    "\n",
    "            if (column_label, row_label, batch_size) in series_dict:\n",
    "                reconstructed_data = series_dict[(column_label, row_label, batch_size)]\n",
    "                sample_mapping, batch_inputs, dummy_inputs, batch_targets, dummy_targets = (\n",
    "                    get_batch_sample_mapping_from_dataframe(reconstructed_data, batch_size)\n",
    "                )\n",
    "                original_x_axis = np.arange(0, batch_inputs.shape[1] + batch_targets.shape[1])\n",
    "                for b, d in enumerate(sample_mapping):\n",
    "                    ax = axes[b] if batch_size > 1 else axes\n",
    "                    ax.plot(original_x_axis[: batch_inputs.shape[1]], batch_inputs[b, :].detach().cpu().numpy())\n",
    "                    ax.plot(original_x_axis[: batch_inputs.shape[1]], dummy_inputs[d, :].detach().cpu().numpy(), linestyle=\"--\")\n",
    "\n",
    "                    ax.plot(original_x_axis[batch_inputs.shape[1] :], batch_targets[b, :].detach().cpu().numpy())\n",
    "                    ax.plot(original_x_axis[batch_inputs.shape[1] :], dummy_targets[d, :].detach().cpu().numpy(), linestyle=\"--\")\n",
    "\n",
    "                    dummy_handle = mlines.Line2D([], [], color=\"none\", label=f\"Sample {b}\")\n",
    "                    ax.legend(\n",
    "                        handles=[dummy_handle],\n",
    "                        loc=legend_loc,\n",
    "                        handlelength=0,\n",
    "                        handletextpad=0,\n",
    "                        fancybox=True,\n",
    "                        fontsize=legend_font_size,\n",
    "                    )\n",
    "                    ax.set_ylim(-0.1, 1.1)\n",
    "                    ax.set_yticks([])\n",
    "                    ax.set_xticks([])\n",
    "\n",
    "            if i == 0:\n",
    "                subfig.text(0.5, 1.0 + column_offset, column_label, ha=\"center\", va=\"top\")\n",
    "            if j == 0:\n",
    "                subfig.text(0.0, 0.5, row_label, ha=\"left\", va=\"center\", fontsize=8, rotation=90)\n",
    "    if external_fig is None:\n",
    "        fig.savefig(f\"./out/plots/reconstructions/grid_reconstructions_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "def gather_run_gradients(dataset_seed, columns, rows, filters, runs, replace_dict={}):\n",
    "    def should_skip_run(run):\n",
    "        if run.config[\"dataset\"] not in dataset_seed or run.config[\"seed\"] != dataset_seed[run.config[\"dataset\"]]:\n",
    "            return True\n",
    "\n",
    "        if \"dummy_grad_list\" not in run.summary or \"split_indexes\" not in run.summary or \"original_grad_list\" not in run.summary:\n",
    "            return True\n",
    "\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) if k in run.config else False for k, v in filter_dict.items()]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def replace_predictor_name(runs_config, name):\n",
    "        run_config_value = runs_config[name]\n",
    "        for key, value in replace_dict.items():\n",
    "            if isinstance(run_config_value, str):\n",
    "                run_config_value = run_config_value.replace(key, value)\n",
    "        if name == \"dataset\":\n",
    "            return f\"{run_config_value} ({dataset_seed[runs_config[name]]})\"\n",
    "        if name == \"model\":\n",
    "            return run_config_value.replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        return run_config_value\n",
    "\n",
    "    gradient_data_dict = {}\n",
    "    if isinstance(runs, list):\n",
    "        runs = [run for run_list in runs for run in run_list]\n",
    "\n",
    "    for run in runs:\n",
    "        if should_skip_run(run):\n",
    "            continue\n",
    "\n",
    "        run_column = replace_predictor_name(run.config, columns)\n",
    "        run_row = replace_predictor_name(run.config, rows)\n",
    "\n",
    "        grad_dict = {\n",
    "            \"dummy_grad_list\": run.summary[\"dummy_grad_list\"],  # List\n",
    "            \"original_grad_list\": run.summary[\"original_grad_list\"],  # List\n",
    "            \"split_indexes\": run.summary[\"split_indexes\"],  # List\n",
    "        }\n",
    "        if (run_column, run_row) not in gradient_data_dict:\n",
    "            gradient_data_dict[(run_column, run_row)] = grad_dict\n",
    "\n",
    "    print(gradient_data_dict.keys())\n",
    "\n",
    "    unique_columns = sorted(list(set([column for column, _ in gradient_data_dict.keys()])))\n",
    "    unique_rows = sorted(list(set([row for _, row in gradient_data_dict.keys()])))\n",
    "\n",
    "    return gradient_data_dict, unique_columns, unique_rows\n",
    "\n",
    "\n",
    "def plot_gradients_in_grid(\n",
    "    gradients_info_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "):\n",
    "    fig, axes = plt.subplots(\n",
    "        len(unique_rows),\n",
    "        len(unique_columns),\n",
    "        figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows)),\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif len(unique_columns) == 1:\n",
    "        axes = np.array([axes]).T\n",
    "\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        min_value, max_value = float(\"inf\"), float(\"-inf\")\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            key = (column_label, row_label)\n",
    "            if key in gradients_info_dict:\n",
    "                grad_dict = gradients_info_dict[key]\n",
    "                grad_diff = np.abs(np.array(grad_dict[\"dummy_grad_list\"]) - np.array(grad_dict[\"original_grad_list\"]))\n",
    "                if (grad_min := grad_diff[grad_diff > 0.0].min()) < min_value:\n",
    "                    min_value = grad_min\n",
    "                if (grad_max := grad_diff.max()) > max_value:\n",
    "                    max_value = grad_max\n",
    "\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            key = (column_label, row_label)\n",
    "            if key in gradients_info_dict:\n",
    "                ax = axes[i, j]\n",
    "                grad_dict = gradients_info_dict[key]\n",
    "                grad_diff = np.abs(np.array(grad_dict[\"dummy_grad_list\"]) - np.array(grad_dict[\"original_grad_list\"]))\n",
    "\n",
    "                split_indexes = grad_dict[\"split_indexes\"]\n",
    "                print(split_indexes)\n",
    "\n",
    "                x_values = np.arange(len(grad_diff))\n",
    "                ax.vlines(x_values, min_value, grad_diff, linewidth=0.05, alpha=0.5)\n",
    "                ax.set_yscale(\"log\")\n",
    "                ax.set_ylim(min_value * 100, max_value)\n",
    "\n",
    "                for split_index in split_indexes:\n",
    "                    ax.axvline(x=split_index, color=\"r\", linestyle=\"--\", linewidth=0.8)\n",
    "\n",
    "                dummy_handle = mlines.Line2D([1], [1], color=\"tab:blue\", label=f\"Gradient Residual\")\n",
    "                ax.legend(\n",
    "                    handles=[dummy_handle],\n",
    "                    loc=legend_loc,\n",
    "                    handlelength=0,\n",
    "                    handletextpad=0,\n",
    "                    fancybox=True,\n",
    "                    fontsize=legend_font_size,\n",
    "                )\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.set_title(column_label)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(row_label)\n",
    "\n",
    "    fig.savefig(f\"./out/plots/reconstructions/grid_gradients_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gradients_histogram_in_grid(\n",
    "    gradients_info_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "):\n",
    "    fig, axes = plt.subplots(\n",
    "        len(unique_rows),\n",
    "        len(unique_columns),\n",
    "        figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows)),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif len(unique_columns) == 1:\n",
    "        axes = np.array([axes]).T\n",
    "\n",
    "    absolute_min = 1e-12\n",
    "    column_min_max = {}\n",
    "\n",
    "    for j, column_label in enumerate(unique_columns):\n",
    "        min_value, max_value = float(\"inf\"), float(\"-inf\")\n",
    "        for row_label in unique_rows:\n",
    "            key = (column_label, row_label)\n",
    "            if key in gradients_info_dict:\n",
    "                grad_dict = gradients_info_dict[key]\n",
    "                grad_diff = np.abs(np.array(grad_dict[\"dummy_grad_list\"]) - np.array(grad_dict[\"original_grad_list\"]))\n",
    "                grad_diff = np.where(grad_diff <= absolute_min, absolute_min, grad_diff)\n",
    "                if (grad_min := grad_diff[grad_diff > 0.0].min()) < min_value:\n",
    "                    min_value = grad_min\n",
    "                if (grad_max := grad_diff.max()) > max_value:\n",
    "                    max_value = grad_max\n",
    "        column_min_max[column_label] = (min_value, max_value)\n",
    "\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            key = (column_label, row_label)\n",
    "            if key in gradients_info_dict:\n",
    "                ax = axes[i, j]\n",
    "                grad_dict = gradients_info_dict[key]\n",
    "                grad_diff = np.abs(np.array(grad_dict[\"dummy_grad_list\"]) - np.array(grad_dict[\"original_grad_list\"]))\n",
    "                grad_diff = np.where(grad_diff == 0, absolute_min, grad_diff)\n",
    "\n",
    "                min_value, max_value = column_min_max[column_label]\n",
    "                bins = np.logspace(np.log10(min_value), np.log10(max_value), 30)\n",
    "                ax.hist(grad_diff, bins=bins, edgecolor=\"black\")\n",
    "                ax.set_xscale(\"log\")\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.set_title(column_label)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(row_label)\n",
    "\n",
    "    fig.savefig(f\"./out/plots/reconstructions/grid_gradients_histograms_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sorted_gradients_in_grid(\n",
    "    gradients_info_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "):\n",
    "    fig, axes = plt.subplots(\n",
    "        len(unique_rows),\n",
    "        len(unique_columns),\n",
    "        figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows)),\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif len(unique_columns) == 1:\n",
    "        axes = np.array([axes]).T\n",
    "\n",
    "    color_map, all_colors = None, None\n",
    "\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            key = (column_label, row_label)\n",
    "            if key in gradients_info_dict:\n",
    "                ax = axes[i, j]\n",
    "                grad_dict = gradients_info_dict[key]\n",
    "                grad_diff = np.abs(np.array(grad_dict[\"dummy_grad_list\"]) - np.array(grad_dict[\"original_grad_list\"]))\n",
    "                split_indexes = grad_dict[\"split_indexes\"]\n",
    "                if color_map is None:\n",
    "                    color_map = plt.cm.get_cmap(\"tab20\", len(split_indexes))\n",
    "                    all_colors = color_map(np.arange(color_map.N))\n",
    "\n",
    "                split_indexes = [0] + split_indexes\n",
    "                sorted_indices = np.argsort(grad_diff)\n",
    "                sorted_grad_diff = grad_diff[sorted_indices]\n",
    "\n",
    "                color_codes = []\n",
    "                split_colors = {\n",
    "                    range_start: all_colors[idx % len(all_colors)] for idx, range_start in enumerate(split_indexes[:-1])\n",
    "                }\n",
    "                for idx in sorted_indices:\n",
    "                    for range_start, range_end in zip(split_indexes[:-1], split_indexes[1:]):\n",
    "                        if range_start <= idx < range_end:\n",
    "                            color_codes.append(split_colors[range_start])\n",
    "                            break\n",
    "                    else:\n",
    "                        color_codes.append(\"tab:gray\")\n",
    "\n",
    "                x_values = np.arange(len(sorted_grad_diff))\n",
    "                for x, y, color in zip(x_values, sorted_grad_diff, color_codes):\n",
    "                    ax.vlines(x, 0, y, color=color, linewidth=0.05, alpha=0.5)\n",
    "\n",
    "                ax.set_yscale(\"log\")\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.set_title(column_label)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(row_label)\n",
    "\n",
    "    # Create a colorbar axis on the right side of the figure\n",
    "    cbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])  # Adjust the position as needed\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=len(split_indexes) - 1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_ticks(np.arange(len(split_indexes) - 1))\n",
    "    cbar.set_ticklabels([f\"Range {split_indexes[idx]}-{split_indexes[idx+1]}\" for idx in range(len(split_indexes) - 1)])\n",
    "    cbar.ax.tick_params(labelsize=legend_font_size)\n",
    "\n",
    "    plt.subplots_adjust(right=0.9)\n",
    "    fig.savefig(f\"./out/plots/reconstructions/grid_sorted_gradients_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "baseline_project_name = \"capsar-meijer/ts-inverse_preparation_baselines\"\n",
    "ts_inverse_project_name = \"capsar-meijer/ts-inverse_preparation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 runs for TS-Inverse and Baselines Comparison\n",
      "Found 72 runs with metrics: {('electricity_370', 'sign', 'DLG-LBFGS'): [[1.999232292175293, 1.9856681823730469], [1.9430434703826904, 1.9910142421722412], [1.9460372924804688, 1.99007511138916]], ('electricity_370', 'sign', 'LTI'): [[0.23065468668937683, 0.10833343863487244], [0.3850267827510834, 0.26284897327423096], [0.3558160662651062, 0.17938193678855896]], ('electricity_370', 'sign', 'DLG-Adam'): [[1.9133542776107788, 1.9255656003952024], [1.929749846458435, 1.9654988050460815], [1.9437367916107176, 1.9631785154342651]], ('electricity_370', 'sign', 'InvG'): [[1.6580898761749268, 0.6611918807029724], [1.6771066188812256, 0.948647141456604], [1.680199384689331, 0.8372775912284851]], ('electricity_370', 'sign', 'DIA'): [[1.6580753326416016, 1.0824203491210938], [1.6771039962768557, 1.3625648021697998], [1.6801718473434448, 1.3370813131332395]], ('electricity_370', 'prune', 'DLG-LBFGS'): [[0.03026920929551125, 0.009130466729402542], [0.0370112843811512, 0.007201302330940962], [0.033877696841955185, 0.010519914329051971]], ('electricity_370', 'prune', 'DLG-Adam'): [[0.0021122663747519255, 0.0090402839705348], [0.005107497796416283, 0.007161056622862816], [0.0028183204121887684, 0.010497119277715685]], ('electricity_370', 'prune', 'LTI'): [[0.06975744664669037, 0.0350671112537384], [0.06329983472824097, 0.05901595577597618], [0.07878676056861877, 0.04571288824081421]], ('electricity_370', 'prune', 'InvG'): [[0.002114395145326853, 0.2130279392004013], [0.005107189994305372, 0.3449558615684509], [0.0028192629106342793, 0.2954676151275635]], ('electricity_370', 'prune', 'DIA'): [[0.002112859394401312, 0.7415190935134888], [0.005106477998197079, 0.8738802671432495], [0.0028197825886309147, 0.9674382209777832]], ('electricity_370', 'gauss', 'LTI'): [[0.23869141936302185, 0.2360325902700424], [0.19770193099975583, 0.13994306325912476], [0.23072469234466553, 0.3175281286239624]], ('electricity_370', 'gauss', 'DLG-LBFGS'): [[1.996702551841736, 1.3522388935089111], [1.9984501600265503, 1.463079810142517], [1.9982271194458008, 1.4341949224472046]], ('electricity_370', 'gauss', 'DLG-Adam'): [[1.9018110036849976, 1.4094791412353516], [1.9136168956756592, 1.455604076385498], [1.9132299423217771, 1.504667043685913]], ('electricity_370', 'gauss', 'InvG'): [[1.7766916751861572, 1.1498501300811768], [1.833851218223572, 1.1766033172607422], [1.8207714557647705, 1.3120616674423218]], ('electricity_370', 'gauss', 'DIA'): [[1.746802568435669, 1.318115472793579], [1.8443430662155151, 1.4013166427612305], [1.8231801986694336, 1.4106128215789795]], ('electricity_370', 'none', 'LTI'): [[0.06089096516370773, 0.03416498005390167], [0.06393035501241684, 0.05195137858390808], [0.08131171762943268, 0.03871762752532959]], ('electricity_370', 'none', 'DLG-LBFGS'): [[0.029471710324287415, 0.00020370687707327304], [0.03529798984527588, 0.0004605029826052487], [0.03430690988898277, 0.0002744317753240466]], ('electricity_370', 'none', 'DLG-Adam'): [[1.689360374257376e-06, 6.22942889094702e-06], [2.457775281072827e-06, 1.8662845832295716e-05], [3.171846287841618e-07, 7.828298294043634e-07]], ('electricity_370', 'none', 'InvG'): [[3.896304406225681e-06, 0.20709088444709775], [5.127721578901401e-06, 0.34175050258636475], [4.10712209486519e-06, 0.2891077399253845]], ('electricity_370', 'none', 'DIA'): [[0.0005272910348139703, 0.7458591461181641], [0.005251938011497259, 0.8769019842147827], [0.005858055781573057, 0.9775561094284058]], ('electricity_370', 'sign', 'TS-Inverse'): [[0.9675599932670592, 1.0376195907592771], [1.0661263465881348, 1.3432554006576538], [1.112238883972168, 1.3078235387802124]], ('electricity_370', 'prune', 'TS-Inverse'): [[0.019699878990650177, 0.008775214664638042], [0.003728378564119339, 0.005874318536370993], [0.01065732818096876, 0.01094758417457342]], ('electricity_370', 'gauss', 'TS-Inverse'): [[0.5394316911697388, 1.005828619003296], [0.5951834917068481, 1.312496542930603], [0.6442012786865234, 1.2182748317718506]], ('electricity_370', 'none', 'TS-Inverse'): [[3.3393895137123764e-05, 1.266349016759705e-07], [3.1304017511502025e-07, 6.391363172042476e-08], [5.859785119355365e-07, 6.34493630968791e-08]]}\n",
      "['electricity_370'] ['gauss', 'none', 'prune', 'sign'] ['DLG-LBFGS', 'DLG-Adam', 'InvG_TV0', 'DIA', 'LTI', 'TS-Inverse']\n",
      "\\begin{table*}[]\n",
      "\\centering\n",
      "\\resizebox{\\linewidth}{!}{\n",
      "\\begin{tabular}{l|l|cc}\n",
      "\\toprule\n",
      "      & Dataset                 & \\multicolumn{2}{c}{Electricity 370} & \\multicolumn{2}{c}{KDDCup} & \\multicolumn{2}{c}{London Smartmeter} & \\multicolumn{2}{c}{Proprietary} \\\\\n",
      "      & REPLACE                 & \\multicolumn{2}{c}{electricity_370} \\\\\n",
      "\\midrule\n",
      "Model &  & Input & Target \\\\\n",
      "\\midrule\n",
      "gauss\t& DLG-LBFGS\t & 1.998$_{0.00}$ & 1.417$_{0.05}$ \\\\\n",
      "\t& DLG-Adam\t & 1.910$_{0.01}$ & 1.457$_{0.04}$ \\\\\n",
      "\t& InvG_TV0\t & N/A & N/A \\\\\n",
      "\t& DIA\t & 1.805$_{0.04}$ & 1.377$_{0.04}$ \\\\\n",
      "\t& LTI\t & \\textbf{0.222$_{0.02}$} & \\textbf{0.231$_{0.07}$} \\\\\n",
      "\t& TS-Inverse\t & 0.593$_{0.04}$ & 1.179$_{0.13}$ \\\\\n",
      "\\midrule\n",
      "none\t& DLG-LBFGS\t & 0.033$_{0.00}$ & 3.1e-04$_{0.00}$ \\\\\n",
      "\t& DLG-Adam\t & \\textbf{1.5e-06$_{0.00}$} & 8.6e-06$_{0.00}$ \\\\\n",
      "\t& InvG_TV0\t & N/A & N/A \\\\\n",
      "\t& DIA\t & 0.004$_{0.00}$ & 0.867$_{0.09}$ \\\\\n",
      "\t& LTI\t & 0.069$_{0.01}$ & 0.042$_{0.01}$ \\\\\n",
      "\t& TS-Inverse\t & 1.1e-05$_{0.00}$ & \\textbf{8.5e-08$_{0.00}$} \\\\\n",
      "\\midrule\n",
      "prune\t& DLG-LBFGS\t & 0.034$_{0.00}$ & 0.009$_{0.00}$ \\\\\n",
      "\t& DLG-Adam\t & \\textbf{0.003$_{0.00}$} & 0.009$_{0.00}$ \\\\\n",
      "\t& InvG_TV0\t & N/A & N/A \\\\\n",
      "\t& DIA\t & 0.003$_{0.00}$ & 0.861$_{0.09}$ \\\\\n",
      "\t& LTI\t & 0.071$_{0.01}$ & 0.047$_{0.01}$ \\\\\n",
      "\t& TS-Inverse\t & 0.011$_{0.01}$ & \\textbf{0.009$_{0.00}$} \\\\\n",
      "\\midrule\n",
      "sign\t& DLG-LBFGS\t & 1.963$_{0.03}$ & 1.989$_{0.00}$ \\\\\n",
      "\t& DLG-Adam\t & 1.929$_{0.01}$ & 1.951$_{0.02}$ \\\\\n",
      "\t& InvG_TV0\t & N/A & N/A \\\\\n",
      "\t& DIA\t & 1.672$_{0.01}$ & 1.261$_{0.13}$ \\\\\n",
      "\t& LTI\t & \\textbf{0.324$_{0.07}$} & \\textbf{0.184$_{0.06}$} \\\\\n",
      "\t& TS-Inverse\t & 1.049$_{0.06}$ & 1.230$_{0.14}$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\caption{}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "experiment_names = [\"ts-inverse_defenses_5-3-2025a\", \"baseline_with_defenses_5-3-2025a\"]\n",
    "runs_baselines = api.runs(baseline_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "print(f\"Found {len(runs_baselines) + len(runs_ts_inverse)} runs for TS-Inverse and Baselines Comparison\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_defenses_5-3-2025a',\n",
    "        'num_attack_steps': lambda x: x == 5000,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == \"baseline_with_defenses_5-3-2025a\",\n",
    "    },\n",
    "\n",
    "]\n",
    "attack_methods_ordered_list = ['DLG-LBFGS', 'DLG-Adam', 'InvG', 'DIA', 'LTI', 'TS-Inverse']\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'kddcup': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 28,\n",
    "}\n",
    "\n",
    "runs = [runs_baselines, runs_ts_inverse]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='defense_name', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='attack_method', filters=filters, runs=runs, \n",
    "                                                                                                         variables_other_sorted=attack_methods_ordered_list)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"baselines_final_18-4-2024\", \"baselines_final_dia_lr_schedular_fix_23-4-2024\"]\n",
    "runs_baselines = api.runs(baseline_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "print(f\"Found {len(runs_baselines)} runs for Baseline Comparison\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_final_18-4-2024',\n",
    "        'attack_method': lambda x: x != 'DIA',\n",
    "        # 'model': lambda x: 'FCN' in x or 'CNN' in x or 'TCN' in x,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_final_dia_lr_schedular_fix_23-4-2024',\n",
    "        'attack_method': lambda x: x == 'DIA',\n",
    "        # 'model': lambda x: 'FCN' in x or 'CNN' in x or 'TCN' in x,\n",
    "    }\n",
    "]\n",
    "# attack_methods_ordered_list = ['DLG-LBFGS', 'DLG-Adam', 'InvG_TV0', 'DIA', 'LTI']\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    # 'kddcup': 10,\n",
    "    # 'london_smartmeter': 10,\n",
    "    # 'tno_electricity': 28,\n",
    "}\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Sm.',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1': 'L1',\n",
    "    'InvG_TV0': 'InvG',\n",
    "}\n",
    "attack_methods_ordered_list = ['DLG-Adam', 'InvG', 'DIA', 'LTI']\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='attack_method', rows='dataset', variables='model', filters=filters, runs=runs_baselines, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, attack_methods_ordered_list, u_rows, u_variables, extra_title='baselines_attack_method_x_dataset_x_model_fcn_cnn_gru_tcn', legend_loc='lower right', plot_size_width=2.5, plot_size_height=0.35)\n",
    "\n",
    "\n",
    "attack_methods_ordered_list = ['DLG-LBFGS', 'DLG-Adam', 'InvG', 'DIA', 'LTI']\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='attack_method', rows='dataset', variables='model', filters=filters, runs=runs_baselines, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, attack_methods_ordered_list, u_rows, u_variables, extra_title='baselines_attack_method_x_dataset_x_model_fcn_cnn_gru_tcn_with_lbfgs', legend_loc='lower right', plot_size_width=2.5, plot_size_height=0.45)\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'kddcup': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 28,\n",
    "}\n",
    "attack_methods_ordered_list = ['DLG-LBFGS', 'DLG-Adam', 'InvG', 'DIA', 'LTI']\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='attack_method', rows='dataset', variables='model', filters=filters, runs=runs_baselines, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, attack_methods_ordered_list, u_rows, u_variables, extra_title='all_baselines_attack_method_x_dataset_x_model_fcn_cnn_gru_tcn', legend_loc='lower right', plot_size_width=2.5, plot_size_height=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"baselines_attacking_seq2seq_24-5-2024\"]\n",
    "runs_baselines = api.runs(baseline_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "print(f\"Found {len(runs_baselines)} runs for Seq-2-Seq Comparison\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_attacking_seq2seq_24-5-2024',\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_attacking_seq2seq_24-5-2024',\n",
    "        'attack_method': lambda x: 'LTI' in x,\n",
    "        'validation_stride': lambda x: x == 1, # Otherwise the dataset is already biased towards only correct interval of day\n",
    "    }\n",
    "]\n",
    "\n",
    "# metric_table, unique_columns, unique_rows, unique_variables = gather_final_metrics_from_reconstructions_by_parameters(columns='dataset', rows='model', \n",
    "#                                                                                                          metric=SMAPELoss, \n",
    "#                                                                                                          variable='attack_method', filters=filters, runs=runs)\n",
    "# print_latex_table_input_target(metric_table, unique_columns, unique_rows, unique_variables, variable_name='Attack Method')\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    # 'kddcup': 10,\n",
    "    # 'london_smartmeter': 28,\n",
    "    'tno_electricity': 28,\n",
    "}\n",
    "replace_dict = {\n",
    "    # 'JitGRU': 'GRU-2-FCN',\n",
    "    'InvG_TV0': 'InvG',\n",
    "    'GRU': 'GRU-2-FCN',\n",
    "    'JitGRU-2-FCN': 'GRU-2-FCN',\n",
    "    'JitSeq2Seq': 'GRU-2-GRU',\n",
    "    'electricity_370': 'Elec. 370',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='model', rows='dataset', variables='attack_method', filters=filters, runs=runs_baselines, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='attacking_seq2seq_24-05-2024', legend_loc='upper right', plot_size_height=0.35, plot_size_width=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"baselines_invg_tv_regularization_ccn_8-6-2024\"]\n",
    "runs_baselines = api.runs(baseline_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "print(f\"Found {len(runs_baselines)} runs for Total Variation Regularization Comparison\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_invg_tv_regularization_ccn_8-6-2024',\n",
    "    },\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='total_variation_alpha_inputs', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='total_variation_beta_targets', filters=filters, runs=runs_baselines)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables)\n",
    "\n",
    "replace_dict = {\n",
    "    'electricity_370': '370',\n",
    "    'tno_electricity': 'Prop.',\n",
    "}\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'tno_electricity': 28,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='total_variation_beta_targets', rows='total_variation_alpha_inputs', variables='dataset', filters=filters, runs=runs_baselines, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='invg_total_variation_effects', legend_loc='lower right', plot_size_height=0.5, plot_size_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table with TS-Inverse and baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"baselines_final_18-4-2024\", \"baselines_final_dia_lr_schedular_fix_23-4-2024\", \"ts-inverse_batch1_with_target_reconst_12-6-2024\", \"ts-inverse_final_cnn_fcn_tcn_with_dummy_init_prior_31-5-2024\", \"ts-inverse_batch1_without_target_reconst_12-6-2024\"]\n",
    "runs_baselines = api.runs(baseline_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}, include_sweeps=False, per_page=100)\n",
    "print(f\"Found {len(runs_baselines) + len(runs_ts_inverse)} runs for TS-Inverse and Baselines Comparison\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_final_18-4-2024',\n",
    "        'attack_method': lambda x: x != 'DIA',\n",
    "        'model': lambda x: 'FCN' in x or 'CNN' in x or 'TCN' in x,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'baselines_final_dia_lr_schedular_fix_23-4-2024',\n",
    "        'attack_method': lambda x: x == 'DIA',\n",
    "        'model': lambda x: 'FCN' in x or 'CNN' in x or 'TCN' in x,\n",
    "    },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_batch1_with_target_reconst_12-6-2024', # L1, FCN, CNN, TCN,\n",
    "    #     'one_shot_targets': lambda x: x == True,\n",
    "    # },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_final_cnn_fcn_tcn_with_dummy_init_prior_31-5-2024',\n",
    "    # }, \n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_batch1_without_target_reconst_12-6-2024',\n",
    "    }\n",
    "\n",
    "]\n",
    "attack_methods_ordered_list = ['DLG-LBFGS', 'DLG-Adam', 'InvG_TV0', 'DIA', 'LTI', 'TS-Inverse']\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'kddcup': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 28,\n",
    "}\n",
    "\n",
    "runs = [runs_baselines, runs_ts_inverse]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='model', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='attack_method', filters=filters, runs=runs, \n",
    "                                                                                                         variables_other_sorted=attack_methods_ordered_list)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"ts-inverse_final_cnn_fcn_tcn_with_dummy_init_prior_31-5-2024\", \"ts-inverse_batch1_with_target_reconst_12-6-2024\", \"ts-inverse_batch1_without_target_reconst_12-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for TS-Inverse Results on Datasets\")\n",
    "\n",
    "filters = [\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_final_gru_mse_learn_dilate_opti_1-5-2024', # GRU Results\n",
    "    #     'inversion_regularization_term': lambda x: x == 0.01,\n",
    "    # },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_final_cnn_fcn_tcn_with_dummy_init_prior_31-5-2024' # L1, Dummy Prior and FCN, CNN, TCN\n",
    "    # },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_final_fcn_cnn_tcn_26-4-2024', # FCN, CNN, TCN Results\n",
    "    #     'gradient_loss': lambda x: x == 'l1',\n",
    "    # }\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_batch1_with_target_reconst_12-6-2024', # L1, FCN, CNN, TCN,\n",
    "        'one_shot_targets': lambda x: x == True,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_batch1_without_target_reconst_12-6-2024',\n",
    "        'one_shot_targets': lambda x: x == False,\n",
    "    }\n",
    "]\n",
    "\n",
    "# attack_methods_ordered_list = ['TS-Inverse']\n",
    "\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary Dataset',\n",
    "    'kddcup': 'KDDCup',\n",
    "}\n",
    "\n",
    "for seed in [10, 43, 28, 80, 71]:\n",
    "    dataset_seed_dict = {\n",
    "        'electricity_370': seed,\n",
    "        'kddcup': seed,\n",
    "        'london_smartmeter': seed,\n",
    "        'tno_electricity': seed,\n",
    "    }\n",
    "    replace_dict = {\n",
    "        'electricity_370': 'Electricity 370',\n",
    "        'london_smartmeter': 'London Smartmeter',\n",
    "        'tno_electricity': 'Proprietary Dataset',\n",
    "        'kddcup': 'KDDCup',\n",
    "        'True': f'With Target ({seed})',\n",
    "        'False': f'Without Target ({seed})',\n",
    "    }\n",
    "\n",
    "    reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='one_shot_targets', variables='model', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "    plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title=f'ts-inverse_dataset_x_model_{seed}', legend_loc='lower right', plot_size_height=0.42, plot_size_width=2.5, column_offset=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def gather_run_quantile_predictions(dataset_seed, columns, rows, variables, filters, runs, replace_dict={}):\n",
    "    def dataframe_keys_and_should_skip_run(run):\n",
    "        if run.config[\"dataset\"] not in dataset_seed or run.config[\"seed\"] != dataset_seed[run.config[\"dataset\"]]:\n",
    "            return [], True\n",
    "\n",
    "        dataframe_keys = []\n",
    "        for key in run.summary.keys():\n",
    "            if \"dataframe_quantile\" in key:\n",
    "                dataframe_keys.append(key)\n",
    "        if len(dataframe_keys) == 0:\n",
    "            return [], True\n",
    "\n",
    "        continue_next_run = [False for _ in range(len(filters))]\n",
    "        for i, filter_dict in enumerate(filters):\n",
    "            if not np.array([v(run.config[k]) for k, v in filter_dict.items() if k in run.config]).all():\n",
    "                continue_next_run[i] = True\n",
    "        if np.array(continue_next_run).all():\n",
    "            return [], True\n",
    "        return dataframe_keys, False\n",
    "\n",
    "    def replace_predictor_name(runs_config, name):\n",
    "        run_config_value = runs_config[name]\n",
    "        for key, value in replace_dict.items():\n",
    "            if isinstance(run_config_value, str):\n",
    "                run_config_value = run_config_value.replace(key, value)\n",
    "        if name == \"dataset\":\n",
    "            return f\"{run_config_value} ({dataset_seed[runs_config[name]]})\"\n",
    "        if name == \"model\":\n",
    "            return run_config_value.replace(\"JitGRU_Predictor\", \"GRU_Predictor\").replace(\"_Predictor\", \"\")\n",
    "        return run_config_value\n",
    "\n",
    "    reconstructed_data_dict = {}\n",
    "    if isinstance(runs, list):\n",
    "        runs = [run for run_list in runs for run in run_list]\n",
    "\n",
    "    quantiles = [-1]\n",
    "    for run in runs:\n",
    "        dataframe_keys, should_skip_run = dataframe_keys_and_should_skip_run(run)\n",
    "        if should_skip_run:\n",
    "            continue\n",
    "\n",
    "        run_column = replace_predictor_name(run.config, columns)\n",
    "        run_row = replace_predictor_name(run.config, rows)\n",
    "        run_variable = replace_predictor_name(run.config, variables)\n",
    "\n",
    "        dataframe_info = run.summary[dataframe_keys[0]]\n",
    "        quantiles = run.config[\"quantiles\"] if \"quantiles\" in run.config else [-1]\n",
    "        file = run.file(dataframe_info[\"path\"])\n",
    "        if not os.path.exists(file.name):\n",
    "            file.download(replace=True)\n",
    "        with open(file.name, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        reconstructed_data = pd.DataFrame(data=data[\"data\"], columns=data[\"columns\"])\n",
    "        if (run_column, run_row, run_variable) not in reconstructed_data_dict:\n",
    "            reconstructed_data_dict[(run_column, run_row, run_variable)] = reconstructed_data\n",
    "\n",
    "    print(reconstructed_data_dict.keys())\n",
    "    if quantiles == [-1]:\n",
    "        print(\"No Quantiles found in the run!\")\n",
    "\n",
    "    unique_columns = sorted(list(set([column for column, _, _ in reconstructed_data_dict.keys()])))\n",
    "    unique_rows = sorted(list(set([row for _, row, _ in reconstructed_data_dict.keys()])))\n",
    "    unique_variables = sorted(list(set([variable for _, _, variable in reconstructed_data_dict.keys()])))\n",
    "\n",
    "    return reconstructed_data_dict, unique_columns, unique_rows, unique_variables, quantiles\n",
    "\n",
    "\n",
    "def plot_single_batch_quantiles_in_grid(\n",
    "    series_dict,\n",
    "    unique_columns,\n",
    "    unique_rows,\n",
    "    batch_size,\n",
    "    quantiles,\n",
    "    extra_title=\"\",\n",
    "    legend_loc=\"upper left\",\n",
    "    column_offset=0.0,\n",
    "    plot_size_width=2,\n",
    "    plot_size_height=0.33,\n",
    "    legend_font_size=7,\n",
    "):\n",
    "    if not series_dict:\n",
    "        print(\"No series given, skipping plotting\")\n",
    "        return\n",
    "    \n",
    "    fig = plt.figure(figsize=(plot_size_width * len(unique_columns), plot_size_height * len(unique_rows) * len(batch_size)))\n",
    "    subfigs = fig.subfigures(len(unique_rows), len(unique_columns))\n",
    "    if len(unique_rows) == 1 and len(unique_columns) == 1:\n",
    "        subfigs = np.array([[subfigs]])\n",
    "    elif len(unique_rows) == 1:\n",
    "        subfigs = np.array([subfigs])\n",
    "    elif len(unique_columns) == 1:\n",
    "        subfigs = np.array([subfigs]).T\n",
    "\n",
    "    colors = plt.cm.tab10.colors\n",
    "    for i, row_label in enumerate(unique_rows):\n",
    "        for j, column_label in enumerate(unique_columns):\n",
    "            subfig = subfigs[i, j]\n",
    "            subfig.subplots_adjust(wspace=0, hspace=0, left=0.05, right=0.99, top=0.95, bottom=0.01)\n",
    "            axes_1 = subfig.subplots(len(batch_size), 1, sharex=True, sharey=True)\n",
    "            if len(batch_size) == 1:\n",
    "                axes_1 = [axes_1]\n",
    "            for k, variable in enumerate(batch_size):\n",
    "                if (column_label, row_label, variable) in series_dict:\n",
    "                    reconstructed_data = series_dict[(column_label, row_label, variable)]\n",
    "                    assert (\n",
    "                        len(reconstructed_data.columns) == (2 * len(quantiles) + 2) * variable\n",
    "                    ), \"Expected (2*len(quantiles) + 2)*variable columns in the dataframe\"\n",
    "\n",
    "                    for b in range(variable):\n",
    "                        df_plot = reconstructed_data[[f\"batch_inputs_{b}_0\", f\"batch_targets_{b}\"]]\n",
    "                        color_offset = 0\n",
    "                        batch_colors = [colors[0 + color_offset], colors[2 + color_offset]]\n",
    "                        df_plot.plot(ax=axes_1[k], legend=False, color=batch_colors, linewidth=0.8)\n",
    "\n",
    "                    q_c_offset = 1\n",
    "                    for q in range(len(quantiles) // 2):\n",
    "                        input_quantiles = reconstructed_data[\n",
    "                            [f\"dummy_quantile_inputs_0_0_{q}\", f\"dummy_quantile_inputs_0_0_{len(quantiles)-q-1}\"]\n",
    "                        ]\n",
    "                        target_quantiles = reconstructed_data[\n",
    "                            [f\"dummy_quantile_targets_0_{q}\", f\"dummy_quantile_targets_0_{len(quantiles)-q-1}\"]\n",
    "                        ]\n",
    "                        # plot between the quantile columns (which are in pairs and opposite)\n",
    "                        axes_1[k].fill_between(\n",
    "                            input_quantiles.index,\n",
    "                            input_quantiles.iloc[:, 0],\n",
    "                            input_quantiles.iloc[:, 1],\n",
    "                            color=colors[q + q_c_offset],\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "                        axes_1[k].fill_between(\n",
    "                            target_quantiles.index,\n",
    "                            target_quantiles.iloc[:, 0],\n",
    "                            target_quantiles.iloc[:, 1],\n",
    "                            color=colors[q + q_c_offset],\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "                        q_c_offset += 3\n",
    "\n",
    "                    q_c_offset = 1\n",
    "                    quantile_handles = []\n",
    "                    for q in range(len(quantiles) // 2):\n",
    "                        quantile_handles.append(\n",
    "                            mpatches.Patch(\n",
    "                                color=colors[q + q_c_offset],\n",
    "                                label=f\"Quantile {quantiles[q]} - {quantiles[len(quantiles)-q-1]}\",\n",
    "                                alpha=0.8,\n",
    "                            )\n",
    "                        )\n",
    "                        q_c_offset += 3\n",
    "                        # quantile_handles.append(mlines.Line2D([], [], color=colors[q], label=f'Quantile {quantiles[q]} - {quantiles[len(quantiles)-q-1]}', linestyle=''))\n",
    "                    axes_1[k].legend(handles=quantile_handles, loc=legend_loc, fancybox=True, fontsize=legend_font_size)\n",
    "\n",
    "                # axes_1[k].set_ylim(-0.1, 1.1)\n",
    "                axes_1[k].set_yticks([])\n",
    "                axes_1[k].set_xticks([])\n",
    "\n",
    "            if i == 0:\n",
    "                subfig.text(0.5, 1.0 + column_offset, column_label, ha=\"center\", va=\"top\")\n",
    "            if j == 0:\n",
    "                subfig.text(0.0, 0.5, row_label, ha=\"left\", va=\"center\", fontsize=8, rotation=90)\n",
    "\n",
    "    fig.savefig(f\"./out/plots/reconstructions/grid_reconstructions_{extra_title}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show the learned priors in terms of quantiles.\n",
    "experiment_names = [\"ts-inverse_quantile_plots_11-6-2024\", \"ts-inverse_quantile_plots_13-6-2024\"]\n",
    "runs_ts_inverse = api.runs(\n",
    "    ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]}\n",
    ")\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Quantile Plots\")\n",
    "\n",
    "replace_dict = {\n",
    "    \"electricity_370\": \"Electricity 370\",\n",
    "    \"london_smartmeter\": \"London Smartmeter\",\n",
    "    \"tno_electricity\": \"Proprietary\",\n",
    "    \"kddcup\": \"KDDCup\",\n",
    "    \"cosine_dia\": \"Cosine\",\n",
    "    \"l1\": \"L1\",\n",
    "    \"euclidean\": \"Euclidean\",\n",
    "    \"1_norm_1_cosine\": \"Cosine + Norm\",\n",
    "    \"JitGRU\": \"GRU-2-FCN\",\n",
    "    \"JitSeq2Seq\": \"GRU-2-GRU\",\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    \"electricity_370\": 10,\n",
    "    \"london_smartmeter\": 10,\n",
    "    # 'tno_electricity': 10,\n",
    "}\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        \"experiment_name\": lambda x: x == \"ts-inverse_quantile_plots_13-6-2024\",\n",
    "        \"model\": lambda x: \"Jit\" not in x and not \"FCN\" in x,\n",
    "        \"dataset\": lambda x: x == \"electricity_370\",\n",
    "    }\n",
    "]\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables, quantiles = gather_run_quantile_predictions(\n",
    "    dataset_seed=dataset_seed_dict,\n",
    "    columns=\"dataset\",\n",
    "    rows=\"model\",\n",
    "    variables=\"batch_size\",\n",
    "    filters=filters,\n",
    "    runs=runs_ts_inverse,\n",
    "    replace_dict=replace_dict,\n",
    ")\n",
    "plot_single_batch_quantiles_in_grid(\n",
    "    reconstruction_dict,\n",
    "    u_columns,\n",
    "    u_rows,\n",
    "    u_variables,\n",
    "    quantiles,\n",
    "    extra_title=\"ts-inverse_ts_learned_quantiles_batch4\",\n",
    "    plot_size_width=5,\n",
    "    plot_size_height=0.8,\n",
    "    legend_loc=\"best\",\n",
    "    column_offset=0.15,\n",
    ")\n",
    "\n",
    "\n",
    "## BELOW IS FOR 1 BATCH and THESIS\n",
    "filters = [\n",
    "    {\n",
    "        \"experiment_name\": lambda x: x == \"ts-inverse_quantile_plots_11-6-2024\",\n",
    "        \"model\": lambda x: \"Jit\" not in x and not \"FCN\" in x,\n",
    "    }\n",
    "]\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables, quantiles = gather_run_quantile_predictions(\n",
    "    dataset_seed=dataset_seed_dict,\n",
    "    columns=\"dataset\",\n",
    "    rows=\"model\",\n",
    "    variables=\"batch_size\",\n",
    "    filters=filters,\n",
    "    runs=runs_ts_inverse,\n",
    "    replace_dict=replace_dict,\n",
    ")\n",
    "plot_single_batch_quantiles_in_grid(\n",
    "    reconstruction_dict,\n",
    "    u_columns,\n",
    "    u_rows,\n",
    "    u_variables,\n",
    "    quantiles,\n",
    "    extra_title=\"ts-inverse_ts_learned_quantiles\",\n",
    "    plot_size_width=3.5,\n",
    "    plot_size_height=0.8,\n",
    "    legend_loc=\"best\",\n",
    "    column_offset=0.15,\n",
    ")\n",
    "\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "dataset_seed_dict = {\n",
    "    \"electricity_370\": 10,\n",
    "    \"london_smartmeter\": 10,\n",
    "    \"tno_electricity\": 10,\n",
    "}\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        \"experiment_name\": lambda x: x == \"ts-inverse_quantile_plots_11-6-2024\",\n",
    "    }\n",
    "]\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables, quantiles = gather_run_quantile_predictions(\n",
    "    dataset_seed=dataset_seed_dict,\n",
    "    columns=\"dataset\",\n",
    "    rows=\"model\",\n",
    "    variables=\"batch_size\",\n",
    "    filters=filters,\n",
    "    runs=runs_ts_inverse,\n",
    "    replace_dict=replace_dict,\n",
    ")\n",
    "plot_single_batch_quantiles_in_grid(\n",
    "    reconstruction_dict,\n",
    "    u_columns,\n",
    "    u_rows,\n",
    "    u_variables,\n",
    "    quantiles,\n",
    "    extra_title=\"ts-inverse_ts_learned_quantiles\",\n",
    "    plot_size_width=3.5,\n",
    "    plot_size_height=0.8,\n",
    "    legend_loc=\"best\",\n",
    "    column_offset=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"ts-inverse_defenses_4-3-2025a\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == \"ts-inverse_defenses_4-3-2025a\",\n",
    "        'model': lambda x: 'FCN' in x,\n",
    "        'dataset': lambda x: x == 'electricity_370',\n",
    "    }\n",
    "]\n",
    "\n",
    "print(len(runs_ts_inverse))\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables, quantiles  = gather_run_quantile_predictions(dataset_seed=dataset_seed_dict, columns='attack_method', rows='defense_name', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_quantiles_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, quantiles, extra_title='ts-inverse_defenses', plot_size_width=5, plot_size_height=0.8, legend_loc='best', column_offset=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regularization\n",
    "\n",
    "Comparison between two regularization techniques using the quantiles. The first one, \"quantile\" uses the pinball loss of the quantiles and the dummy data. The \"quantile$_{\\text{bounds}}$\" regularizes the dummy data with the L1 loss if the value is outside the quantile bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the learned priors in terms of quantiles.\n",
    "experiment_names = [\"ts-inverse_quantile_bounds_regularization_11-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Learned Prior Regularization\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_quantile_bounds_regularization_11-6-2024',\n",
    "        'inversion_regularization_loss': lambda x: x == 'quantile_bounds',\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='inversion_regularization_term_inputs', rows='dataset', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='inversion_regularization_term_targets', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Loss')\n",
    "\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_quantile_bounds_regularization_11-6-2024',\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='inversion_regularization_term_inputs', rows='inversion_regularization_term_targets', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='inversion_regularization_loss', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Loss')\n",
    "\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1': 'L1',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm'\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_quantile_bounds_regularization_11-6-2024',\n",
    "        'inversion_regularization_loss': lambda x: x == 'quantile_bounds',\n",
    "    }\n",
    "]\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='inversion_regularization_term_targets', rows='inversion_regularization_term_inputs', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_learned_quantile_bounds_regularization', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "# ## ^^ IN THE PAPER\n",
    "\n",
    "# # ## BELOW IS FOR THESIS\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_quantile_bounds_regularization_11-6-2024',\n",
    "        'inversion_regularization_loss': lambda x: x == 'quantile',\n",
    "    }\n",
    "]\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='inversion_regularization_term_targets', rows='inversion_regularization_term_inputs', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_learned_quantile_regularization', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS Regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodicity Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_ts_regularization_11-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for TS Regularization\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'trend_term': lambda x: x == 0,\n",
    "        'periodicity_loss': lambda x: 'mean' in x,\n",
    "    }\n",
    "]\n",
    "\n",
    "# metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='periodicity_loss', \n",
    "#                                                                                                          metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "#                                                                                                          variable='periodicity_term', filters=filters, runs=runs_ts_inverse)\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_from_reconstructions_by_parameters(columns='dataset', rows='periodicity_loss', \n",
    "                                                                                                         metric=SMAPELoss,\n",
    "                                                                                                         variable='periodicity_term', filters=filters, runs=runs_ts_inverse)\n",
    "\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Reg. Term.')\n",
    "\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'trend_term': lambda x: x == 0,\n",
    "        'periodicity_loss': lambda x: x == 'l1_mean',\n",
    "        'dataset': lambda x: x == 'electricity_370',\n",
    "        'periodicity_term': lambda x: x == 2 or x == 0,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'trend_term': lambda x: x == 0,\n",
    "        'periodicity_loss': lambda x: x == 'l1_mean',\n",
    "        'dataset': lambda x: x == 'london_smartmeter',\n",
    "        'periodicity_term': lambda x: x == 0.5 or x == 0,\n",
    "    },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "    #     'trend_loss': lambda x: x == 'l1_mean',\n",
    "    #     'trend_term': lambda x: x == 0,\n",
    "    #     'periodicity_loss': lambda x: x == 'l1_mean',\n",
    "    #     'dataset': lambda x: x == 'tno_electricity',\n",
    "    #     'periodicity_term': lambda x: x == 1,\n",
    "    # }\n",
    "]\n",
    "replace_dict = {\n",
    "    'l1_mean': '',\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 43,\n",
    "    'london_smartmeter': 10,\n",
    "}\n",
    "def plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict):\n",
    "    # Create multiple subfigs next to each other and add the plots to the subfigs\n",
    "    fig = plt.figure(figsize=(3*len(dataset_seed_dict), 3.8))\n",
    "    subfigs = fig.subfigures(1, len(dataset_seed_dict))\n",
    "    for i, (dataset, seed) in enumerate(dataset_seed_dict.items()):\n",
    "        reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed={dataset: seed}, columns='periodicity_loss', rows='periodicity_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "        subfigs[i] = plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_periodicity_regularization_specific', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best', external_fig=subfigs[i])\n",
    "        subfigs[i].text(0.5, 1.01, f\"{replace_dict[dataset]} ({seed})\", ha='center', va='top')\n",
    "    fig.savefig(f'./out/plots/reconstructions/grid_reconstructions_ts-inverse_ts_periodicity_regularization_specific.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict)\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'trend_term': lambda x: x == 0,\n",
    "        'periodicity_loss': lambda x: 'mean' in x,\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1_mean': 'L1',\n",
    "    'l2_mean': 'L2',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm'\n",
    "}\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='periodicity_term', rows='periodicity_loss', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_periodicity_regularization', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 10,\n",
    "}\n",
    "#PLOT ALL TERMS AND LOSSES ON DATASETS (specific seeds)\n",
    "def plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict):\n",
    "    # Create multiple subfigs next to each other and add the plots to the subfigs\n",
    "    fig = plt.figure(figsize=(5*len(dataset_seed_dict), 10))\n",
    "    subfigs = fig.subfigures(1, len(dataset_seed_dict))\n",
    "    for i, (dataset, seed) in enumerate(dataset_seed_dict.items()):\n",
    "        reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed={dataset: seed}, columns='periodicity_loss', rows='periodicity_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "        subfigs[i] = plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_periodicity_regularization_large', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best', external_fig=subfigs[i])\n",
    "        subfigs[i].text(0.5, 1.01, f\"{replace_dict[dataset]} ({seed})\", ha='center', va='top')\n",
    "    fig.savefig(f'./out/plots/reconstructions/grid_reconstructions_ts-inverse_ts_periodicity_regularization_large.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_ts_regularization_11-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for TS Regularization\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: not 'sum' in x,\n",
    "        'periodicity_term': lambda x: x == 0,\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='trend_loss', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='trend_term', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Reg. Term.')\n",
    "\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'dataset': lambda x: x == 'electricity_370',\n",
    "        'trend_term': lambda x: x == 2,\n",
    "    },\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: x == 'l1_mean',\n",
    "        'dataset': lambda x: x == 'london_smartmeter',\n",
    "        'trend_term': lambda x: x == 0.5,\n",
    "    },\n",
    "    # {\n",
    "    #     'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "    #     'trend_loss': lambda x: x == 'l1_mean',\n",
    "    #     'dataset': lambda x: x == 'tno_electricity',\n",
    "    #     'trend_term': lambda x: x == 1,\n",
    "    # }\n",
    "]\n",
    "\n",
    "replace_dict = {\n",
    "    'l1_mean': '',\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 43,\n",
    "    'london_smartmeter': 10,\n",
    "}\n",
    "def plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict):\n",
    "    # Create multiple subfigs next to each other and add the plots to the subfigs\n",
    "    fig = plt.figure(figsize=(3*len(dataset_seed_dict), 1.8))\n",
    "    subfigs = fig.subfigures(1, len(dataset_seed_dict))\n",
    "    for i, (dataset, seed) in enumerate(dataset_seed_dict.items()):\n",
    "        reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed={dataset: seed}, columns='trend_loss', rows='trend_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "        subfigs[i] = plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_trend_regularization_specific', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best', external_fig=subfigs[i])\n",
    "        subfigs[i].text(0.5, 1.01, f\"{replace_dict[dataset]} ({seed})\", ha='center', va='top')\n",
    "    fig.savefig(f'./out/plots/reconstructions/grid_reconstructions_ts-inverse_ts_trend_regularization_specific.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict)\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_ts_regularization_11-6-2024',\n",
    "        'trend_loss': lambda x: not 'sum' in x,\n",
    "        'periodicity_term': lambda x: x == 0,\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1_mean': 'L1',\n",
    "    'l2_mean': 'L2',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm'\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='trend_term', rows='trend_loss', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_trend_regularization', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "# ## ^^ IN THE PAPER\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 10,\n",
    "}\n",
    "# ## LARGE PLOT FOR THESIS\n",
    "def plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict):\n",
    "    # Create multiple subfigs next to each other and add the plots to the subfigs\n",
    "    fig = plt.figure(figsize=(5*len(dataset_seed_dict), 10))\n",
    "    subfigs = fig.subfigures(1, len(dataset_seed_dict))\n",
    "    for i, (dataset, seed) in enumerate(dataset_seed_dict.items()):\n",
    "        reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed={dataset: seed}, columns='trend_loss', rows='trend_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "        subfigs[i] = plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_trend_regularization_large', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best', external_fig=subfigs[i])\n",
    "        subfigs[i].text(0.5, 1.01, f\"{replace_dict[dataset]} ({seed})\", ha='center', va='top')\n",
    "    fig.savefig(f'./out/plots/reconstructions/grid_reconstructions_ts-inverse_ts_trend_regularization_large.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_multiple_larger_columns_multi_batch_next_to_each_other(dataset_seed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Regularizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_combinations_regularization_12-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for TS Regularization Tuning\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_combinations_regularization_12-6-2024',\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='periodicity_term', rows='trend_term', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='attack_method', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Reg. Term.')\n",
    "\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1_': 'L1 ',\n",
    "    'L2_': 'L2 ',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm',\n",
    "    'TS-Inverse_Trend_Periodicity_Learned': '',\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_combinations_regularization_12-6-2024',\n",
    "        'attack_method': lambda x: x == 'TS-Inverse_Trend_Periodicity_Learned',\n",
    "        'trend_term': lambda x: x == 0.5,\n",
    "        'periodicity_term': lambda x: x == 1,\n",
    "    }\n",
    "]\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='attack_method', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_regularization_combined_trend_periodicity_learned_10', plot_size_width=2.5, plot_size_height=0.5, legend_loc='lower right')\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='attack_method', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_regularization_combined_trend_periodicity_learned_43', plot_size_width=2.5, plot_size_height=0.5, legend_loc='lower right')\n",
    "\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_combinations_regularization_12-6-2024',\n",
    "        'attack_method': lambda x: x == 'TS-Inverse_Trend_Periodicity',\n",
    "    }\n",
    "]\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='trend_term', rows='periodicity_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_regularization_combined_trend_periodicity', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_combinations_regularization_12-6-2024',\n",
    "        'attack_method': lambda x: x == 'TS-Inverse_Trend_Periodicity_Learned',\n",
    "    }\n",
    "]\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='trend_term', rows='periodicity_term', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_ts_regularization_combined_trend_periodicity_learned', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Loss Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_fixed_gradient_loss_12-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Gradient Loss Comparisons\")\n",
    "\n",
    "\n",
    "# IN THE PAPER\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_fixed_gradient_loss_12-6-2024',\n",
    "        'model': lambda x: 'TCN' in x,\n",
    "        # 'seed': lambda x: x == 10  or x == 43,\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='batch_size', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='gradient_loss', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Gradient Loss')\n",
    "\n",
    "\n",
    "# filters = [\n",
    "#     {\n",
    "#         'experiment_name': lambda x: x == 'ts-inverse_fixed_gradient_loss_12-6-2024',\n",
    "#         'dataset': lambda x: x == 'electricity_370',\n",
    "#         'seed': lambda x: x == 10  or x == 43,\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='model', rows='batch_size', \n",
    "#                                                                                                          metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "#                                                                                                          variable='gradient_loss', filters=filters, runs=runs_ts_inverse)\n",
    "# print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Gradient Loss')\n",
    "\n",
    "# filters = [\n",
    "#     {\n",
    "#         'experiment_name': lambda x: x == 'ts-inverse_fixed_gradient_loss_12-6-2024',\n",
    "#         'dataset': lambda x: x == 'london_smartmeter',\n",
    "#         'seed': lambda x: x == 10  or x == 43,\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='model', rows='batch_size', \n",
    "#                                                                                                          metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "#                                                                                                          variable='gradient_loss', filters=filters, runs=runs_ts_inverse)\n",
    "# print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Gradient Loss')\n",
    "\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_fixed_gradient_loss_12-6-2024',\n",
    "        'batch_size': lambda x: x == 1,\n",
    "        'model': lambda x: 'TCN' in x,\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    '1_l1norm_1_cosine': 'Cosine + L1-Norm',\n",
    "    'l1': 'L1-Norm',\n",
    "    'euclidean': 'L2-Norm',\n",
    "    '1_l2norm_1_cosine': 'Cosine + L2-Norm'\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 28,\n",
    "    # 'tno_electricity': 43,\n",
    "}\n",
    "sorted_variables = ['Cosine + L1-Norm', 'Cosine + L2-Norm', 'Cosine', 'L2-Norm', 'L1-Norm']\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='model', variables='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, sorted_variables, extra_title='ts-inverse_tcn_x_5gradient_loss_x_dataset', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "# ## ^^ IN THE PAPER\n",
    "\n",
    "\n",
    "# ## BELOW IS FOR THESIS\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='model', variables='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, sorted_variables, extra_title='ts-inverse_tcn_x_4gradient_loss_x_all_dataset', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU-2-FCN and GRU-2-GRU architectures as defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_defense_13-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Defense Comparisons\")\n",
    "\n",
    "# IN THE PAPER\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_defense_13-6-2024',\n",
    "        'warmup_number_of_batches': lambda x: x == 0,\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='model', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='batch_size', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='$\\mathcal{B}$')\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_defense_13-6-2024',\n",
    "        'warmup_number_of_batches': lambda x: x == 0,\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'TS-Inverse_Trend_Periodicity_Learned': 'TS-Inverse',\n",
    "    'JitGRU': 'GRU-2-FCN',\n",
    "    'JitSeq2Seq': 'GRU-2-GRU',\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'kddcup': 10,\n",
    "    'tno_electricity': 10,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='model', variables='attack_method', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_gru_defense', plot_size_width=3, plot_size_height=0.8, legend_loc='best', column_offset=0.08)\n",
    "# ## ^^ IN THE PAPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Gradient Loss without target reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PAPER\n",
    "experiment_names = [\"ts-inverse_final_tcn_x_elec_data_x_gradient_loss_5-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Gradient Loss Comparisons\")\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_tcn_x_elec_data_x_gradient_loss_5-6-2024',\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='dataset', rows='batch_size', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='gradient_loss', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Gradient Loss')\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_tcn_x_elec_data_x_gradient_loss_5-6-2024',\n",
    "        'batch_size': lambda x: x == 1\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1': 'L1',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm'\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'tno_electricity': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='model', variables='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_tcn_x_4gradient_loss_x_dataset', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "## ^^ IN THE PAPER\n",
    "\n",
    "\n",
    "## BELOW IS FOR THESIS\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='model', variables='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_tcn_x_4gradient_loss_x_all_dataset', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_tcn_x_elec_data_x_gradient_loss_5-6-2024',\n",
    "        'batch_size': lambda x: x == 4\n",
    "    }\n",
    "]\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "    'london_smartmeter': 10,\n",
    "    'tno_electricity': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='dataset', rows='gradient_loss', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_tcn_batch4_x_gradient_loss_x_dataset', plot_size_width=2.5, plot_size_height=0.45, legend_loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN and CNN Gradient loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"ts-inverse_final_tcn_x_elec_data_x_gradient_loss_5-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for FCN and CNN Gradient Loss Comparisons\")\n",
    "\n",
    "# NOT IN THE PAPER (BECAUSE LESS STRONG CASE FOR CNN and FCN) Can be PUT IN THE THESIS\n",
    "# DOES NOT USE PRIOR PREDICTION\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'gradient_loss': lambda x: x in ['1_norm_1_cosine', 'cosine_dia', 'euclidean', 'l1'],\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_table, u_columns, u_rows, u_variables = gather_final_metrics_by_parameters(columns='model', rows='batch_size', \n",
    "                                                                                                         metrics=['inputs/smape/mean', 'targets/smape/mean'], \n",
    "                                                                                                         variable='gradient_loss', filters=filters, runs=runs_ts_inverse)\n",
    "print_latex_table_input_target(metric_table, u_columns, u_rows, u_variables, variable_name='Gradient Loss')\n",
    "\n",
    "\n",
    "# Plotting specific example for in the paper, showing target difference between L1 and Cosine\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'gradient_loss': lambda x: x in ['1_norm_1_cosine', 'cosine_dia', 'euclidean', 'l1'],\n",
    "        'batch_size': lambda x: x == 1\n",
    "    }\n",
    "]\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'l1': 'L1',\n",
    "    'euclidean': 'Euclidean',\n",
    "    '1_norm_1_cosine': 'Cosine + Norm'\n",
    "}\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='gradient_loss', rows='dataset', variables='model', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_gradient_loss_x_model_x_electricity_370', plot_size_width=2.5, plot_size_height=0.45, legend_loc='center')\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 28,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='gradient_loss', rows='dataset', variables='model', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_gradient_loss_x_model_x_electricity_370', plot_size_width=2.5, plot_size_height=0.45, legend_loc='center')\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 43,\n",
    "}\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='gradient_loss', rows='dataset', variables='model', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_single_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_gradient_loss_x_model_x_electricity_370', plot_size_width=2.5, plot_size_height=0.45, legend_loc='center')\n",
    "\n",
    "\n",
    "# Plotting specific example for in the paper, showing target difference between L1 and Cosine\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'gradient_loss': lambda x: x in ['1_norm_1_cosine', 'cosine_dia', 'euclidean', 'l1'],\n",
    "        'batch_size': lambda x: x == 4\n",
    "    }\n",
    "]\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "\n",
    "reconstruction_dict, u_columns, u_rows, u_variables  = gather_run_reconstructions(dataset_seed=dataset_seed_dict, columns='gradient_loss', rows='model', variables='batch_size', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_multi_batch_reconstructions_in_grid(reconstruction_dict, u_columns, u_rows, u_variables, extra_title='ts-inverse_batch_4_gradient_loss_x_model_x_electricity_370', plot_size_width=2.5, plot_size_height=0.45, legend_loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\"ts-inverse_final_gradient_loss_4-6-2024\"]\n",
    "runs_ts_inverse = api.runs(ts_inverse_project_name, filters={\"$or\": [{\"config.experiment_name\": name} for name in experiment_names]})\n",
    "print(f\"Found {len(runs_ts_inverse)} runs for Gradient Plots Comparisons\")\n",
    "\n",
    "\n",
    "dataset_seed_dict = {\n",
    "    'electricity_370': 10,\n",
    "}\n",
    "replace_dict = {\n",
    "    'electricity_370': 'Electricity 370',\n",
    "    'london_smartmeter': 'London Smartmeter',\n",
    "    'tno_electricity': 'Proprietary',\n",
    "    'kddcup': 'KDDCup',\n",
    "    'cosine_dia': 'Cosine',\n",
    "    'euclidean': 'L2',\n",
    "    '1_norm_1_cosine': 'Cosine + L2-Norm',\n",
    "    '1_l1_1_cosine': 'Cosine + L1-Norm',\n",
    "    'l1': 'L1',\n",
    "}\n",
    "\n",
    "sorted_columns = ['Cosine', 'L1', 'L2', 'Cosine + L1-Norm', 'Cosine + L2-Norm']\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'model': lambda x: 'FCN' in x,\n",
    "        'batch_size': lambda x: x == 1,\n",
    "        'gradient_loss': lambda x: x != '1_inorm_1_icosine' and x != 'l1_skip_1D',\n",
    "    }\n",
    "]\n",
    "gradients_dict, u_columns, u_rows  = gather_run_gradients(dataset_seed=dataset_seed_dict, columns='dataset', rows='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_gradients_histogram_in_grid(gradients_dict, u_columns, u_rows, extra_title='ts-inverse_fcn_gradient_loss', plot_size_width=4, plot_size_height=2, legend_loc='upper left', legend_font_size=8)\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'model': lambda x: 'CNN' in x,\n",
    "        'batch_size': lambda x: x == 1,\n",
    "        'gradient_loss': lambda x: x != '1_inorm_1_icosine' and x != 'l1_skip_1D',\n",
    "    }\n",
    "]\n",
    "gradients_dict, u_columns, u_rows  = gather_run_gradients(dataset_seed=dataset_seed_dict, columns='dataset', rows='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_gradients_histogram_in_grid(gradients_dict, u_columns, u_rows, extra_title='ts-inverse_cnn_gradient_loss', plot_size_width=4, plot_size_height=2, legend_loc='upper left', legend_font_size=8)\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        'experiment_name': lambda x: x == 'ts-inverse_final_gradient_loss_4-6-2024',\n",
    "        'model': lambda x: 'TCN' in x,\n",
    "        'batch_size': lambda x: x == 1,\n",
    "        'gradient_loss': lambda x: x != '1_inorm_1_icosine' and x != 'l1_skip_1D',\n",
    "    }\n",
    "]\n",
    "gradients_dict, u_columns, u_rows  = gather_run_gradients(dataset_seed=dataset_seed_dict, columns='dataset', rows='gradient_loss', filters=filters, runs=runs_ts_inverse, replace_dict=replace_dict)\n",
    "plot_gradients_histogram_in_grid(gradients_dict, u_columns, u_rows, extra_title='ts-inverse_tcn_gradient_loss', plot_size_width=4, plot_size_height=2, legend_loc='upper left', legend_font_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
